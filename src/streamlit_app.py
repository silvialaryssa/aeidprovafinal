import csv
from xml.parsers.expat import model
from sklearn import pipeline
import streamlit as st
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import numpy as np
from sklearn.linear_model import LassoCV
import altair as alt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.stats.diagnostic import het_breuschpagan
from scipy.stats import shapiro
import statsmodels.api as sm
import streamlit as st
import pandas as pd
import plotly.express as px

# ================================
# üìÜ FUN√á√ïES UTILIT√ÅRIAS
# ================================



@st.cache_data
def carregar_dados(uploaded_file=None):
    DEFAULT_FILE = 'src/kc_house_data.csv'
    if uploaded_file is not None:
        return pd.read_csv(uploaded_file)
    else:
        return pd.read_csv(DEFAULT_FILE)


def avaliar_pressupostos(X, y_true, y_pred):
  #########################
    # Supondo que voc√™ j√° tenha y_pred e residuos como arrays ou Series
    df_residuos = pd.DataFrame({
        "Valores previstos": y_pred,
        "Res√≠duos": y_true - y_pred  # ou residuos, se j√° estiver pronto
    })

    # Gr√°fico de dispers√£o com linha horizontal em y=0
    scatter = alt.Chart(df_residuos).mark_circle(size=60, opacity=0.6).encode(
        x=alt.X("Valores previstos", title="Valores previstos"),
        y=alt.Y("Res√≠duos", title="Res√≠duos"),
        tooltip=["Valores previstos", "Res√≠duos"]
    ).properties(
        width=400,
        height=300,
        title="Res√≠duos vs Valores previstos"
    )

    linha_zero = alt.Chart(pd.DataFrame({"y": [0]})).mark_rule(color='red', strokeDash=[5,5]).encode(
        y='y'
    )

    # Combine scatter + linha zero
    chart = scatter + linha_zero

    st.altair_chart(chart, use_container_width=True)
    
  #######################

def ajustar_modelo_lasso(X,y):
    """
    Ajusta um modelo de regress√£o Lasso com valida√ß√£o cruzada.
    Retorna o modelo treinado e os coeficientes.
    """
    st.markdown("### üîç Ajuste do Modelo com Lasso (com valida√ß√£o cruzada)")

    # Ajusta o modelo com valida√ß√£o cruzada para escolher o melhor alpha
    lasso = LassoCV(cv=5, random_state=42).fit(X, y)
    coef_lasso = pd.Series(lasso.coef_, index=X.columns)

    # Exibe os resultados
    st.write(f"Melhor alpha (Œª): {lasso.alpha_:.6f}")
    st.markdown("### üìå Coeficientes do Lasso")
    st.dataframe(coef_lasso[coef_lasso != 0].sort_values(ascending=False))

    return lasso


# =================
# Fun√ß√£o para Remover Outliers com IQR
# =================
def remover_outliers_iqr(df, colunas=None, threshold=1.5):
    """
    Removes outliers from numeric columns based on the IQR method.
    Parameters:
        df: Original DataFrame
        colunas: list of numeric columns to evaluate (if None, applies to all)
        threshold: IQR multiplier to define limits (default: 1.5)
    Returns:
        DataFrame without outliers
    """
    if colunas is None:
        colunas = df.select_dtypes(include='number').columns.tolist()

    df_sem_outliers = df.copy()
    for col in colunas:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        limite_inferior = Q1 - threshold * IQR
        limite_superior = Q3 + threshold * IQR
        df_sem_outliers = df_sem_outliers[
            (df_sem_outliers[col] >= limite_inferior) & (df_sem_outliers[col] <= limite_superior)
        ]

    return df_sem_outliers




# =============================
# üîß Fun√ß√µes - Quest√£o 1
# =============================
def q1_etapa1():
    st.markdown("---")
    def exibir_dicionario_variaveis():
        st.markdown("### Dicion√°rio de Vari√°veis")
        st.markdown("""
        | Coluna            | Tradu√ß√£o                        | Descri√ß√£o                                                                 |
        |-------------------|----------------------------------|---------------------------------------------------------------------------|
        | `id`              | Identificador                   | ID √∫nico do im√≥vel                                                        |
        | `date`            | Data da venda                   | Data em que o im√≥vel foi vendido                                          |
        | `price`           | Pre√ßo                           | Pre√ßo de venda do im√≥vel (vari√°vel alvo)                                 |
        | `bedrooms`        | Quartos                         | N√∫mero de quartos                                                         |
        | `bathrooms`       | Banheiros                       | N√∫mero de banheiros (valores podem ser fracionados)                      |
        | `sqft_living`     | √Årea √∫til (p√©s¬≤)                | √Årea interna utiliz√°vel do im√≥vel                                        |
        | `sqft_lot`        | √Årea do terreno (p√©s¬≤)         | Tamanho do lote do im√≥vel                                                |
        | `floors`          | Andares                         | N√∫mero de andares                                                         |
        | `waterfront`      | Frente para o mar               | 1 se o im√≥vel tem vista para o mar; 0 caso contr√°rio                      |
        | `view`            | Visibilidade/Vis√£o              | Grau de qualidade da vista (0‚Äì4)                                          |
        | `condition`       | Condi√ß√£o                        | Condi√ß√£o geral do im√≥vel (1‚Äì5)                                            |
        | `grade`           | Qualidade de constru√ß√£o         | Avalia√ß√£o da qualidade da constru√ß√£o (1‚Äì13)                               |
        | `sqft_above`      | √Årea acima do solo (p√©s¬≤)       | √Årea constru√≠da acima do solo                                            |
        | `sqft_basement`   | √Årea do por√£o (p√©s¬≤)            | √Årea do por√£o                                                             |
        | `yr_built`        | Ano de constru√ß√£o               | Ano original de constru√ß√£o                                                |
        | `yr_renovated`    | Ano de reforma                  | Ano da √∫ltima reforma (0 se nunca reformado)                             |
        | `zipcode`         | CEP                             | C√≥digo postal                                                             |
        | `lat`             | Latitude                        | Coordenada de latitude                                                    |
        | `long`            | Longitude                       | Coordenada de longitude                                                   |
        | `sqft_living15`   | √Årea √∫til dos vizinhos (p√©s¬≤)   | M√©dia da √°rea √∫til das 15 casas mais pr√≥ximas                             |
        | `sqft_lot15`      | √Årea dos terrenos vizinhos      | M√©dia do tamanho dos lotes das 15 casas mais pr√≥ximas                     |
            """)
        
    st.header("Q1 - 1 - Regress√£o Linear - An√°lise Descritiva dos Dados")
    exibir_dicionario_variaveis()
    uploaded_file = 'src/kc_house_data.csv'

    df = carregar_dados(uploaded_file)
    st.session_state["kc_df"] = df
    #st.success("‚úÖ Arquivo carregado com sucesso!")

    #st.markdown("### üîç Preview dos Dados")
    #st.dataframe(df.head())
    #st.dataframe(df)

    st.markdown("### Estat√≠sticas Descritivas")
    st.dataframe(df.describe())

    #st.markdown("### Mediana das Vari√°veis Num√©ricas")
    #st.dataframe(df.median(numeric_only=True))

    if "price" in df.columns:
        # excluindo as variaves id e zipcod
        #st.markdown("### Correla√ß√£o com o Pre√ßo (`price`)")
        #correlacoes = df.drop(columns=["id", "zipcode"], errors='ignore').corr(numeric_only=True)["price"].sort_values(ascending=False)
        #st.write(correlacoes)

        st.markdown("### Mapa de Correla√ß√£o")
        st.subheader("Interpreta√ß√£o do Mapa de Correla√ß√£o")

        st.markdown("""
        O mapa mostra que **√°rea √∫til (`sqft_living`)** e **qualidade da constru√ß√£o (`grade`)** s√£o as vari√°veis mais fortemente ligadas ao pre√ßo dos im√≥veis.  
        Por outro lado, vari√°veis como **condi√ß√£o (`condition`)** e **ano de constru√ß√£o (`yr_built`)** apresentam baixa correla√ß√£o linear com o pre√ßo, o que sugere uma influ√™ncia limitada na modelagem por regress√£o linear.

        Al√©m disso, algumas vari√°veis est√£o fortemente correlacionadas entre si, como `sqft_living` e `sqft_above`, indicando a necessidade de aten√ß√£o √† **multicolinearidade** ao construir o modelo preditivo.
        """)

        # Plotar o mapa de correla√ß√£o
        plt.figure(figsize=(10, 8))
        # excluindo as variaves id e zipcod
        sns.heatmap(df.drop(columns=["id"], errors='ignore').corr(numeric_only=True), annot=True, fmt=".2f", cmap="coolwarm")
        st.pyplot(plt.gcf())
        plt.clf()
    
  
        st.markdown(r"""
        A base de dados apresenta **21.613 im√≥veis** registrados na regi√£o de King County, nos EUA, com **21 vari√°veis** sobre caracter√≠sticas f√≠sicas, localiza√ß√£o e pre√ßos.

        ### üîπ Vari√°vel Alvo: `price`
        - **M√©dia**: aproximadamente \$540.088  
        - **Mediana**: \$450.000  
        - **Desvio padr√£o**: \$367.127  
        - A distribui√ß√£o √© **assim√©trica √† direita**, indicando a presen√ßa de im√≥veis de alto padr√£o que elevam a m√©dia.
        """)



    
    # ----------- HISTOGRAMAS INTERATIVOS -----------
    st.subheader("üîπ Histogramas das Vari√°veis")

    cols_hist = ['price', 'sqft_living', 'bedrooms', 'bathrooms', 'floors', 'sqft_above', 'sqft_basement']

    selected_hist = st.multiselect("Selecione vari√°veis num√©ricas para visualizar histogramas:", options=cols_hist)

    if not selected_hist:
        st.info("Selecione ao menos uma vari√°vel para visualizar.")
    else:
        for col in selected_hist:
            st.subheader(f"Distribui√ß√£o de {col}")
            chart = alt.Chart(df).mark_bar(opacity=0.7, color='steelblue').encode(
                alt.X(col, bin=alt.Bin(maxbins=40), title=col),
                y='count()',
                tooltip=[col]
            ).properties(width=600, height=300)
            st.altair_chart(chart, use_container_width=True)

    # ----------- BOXPLOTS INTERATIVOS -----------
    st.header("üì¶ Boxplots: Pre√ßo vs Vari√°veis Categ√≥ricas")

    cols_box = ['bedrooms', 'bathrooms', 'floors']
    selected_box = st.multiselect("Selecione vari√°veis para comparar com o pre√ßo:", options=cols_box)

    if not selected_box:
        st.info("Selecione ao menos uma vari√°vel para visualizar boxplots.")
    else:
        for col in selected_box:
            st.subheader(f"Pre√ßo vs {col}")
            chart = alt.Chart(df).mark_boxplot(extent='min-max').encode(
                x=alt.X(f'{col}:O', title=col),
                y=alt.Y('price:Q', scale=alt.Scale(type='log'), title='Pre√ßo (escala log)'),
                tooltip=['price', col]
            ).properties(width=600, height=300)
            st.altair_chart(chart, use_container_width=True)

# =============================

#
def q1_etapa2():
    st.markdown("---")
    st.header("Q1 - 2 - Regressao lienar Modelo de Regress√£o Linear")
    st.info("Crie o modelo de regress√£o linear com m√©tricas de desempenho.")

    df = st.session_state.get("kc_df")
    if df is None:
        st.warning("‚ö†Ô∏è Os dados ainda n√£o foram carregados. Execute a Etapa 1 primeiro.")
        return

    df = df.drop(columns=["id", "date", "zipcode"], errors='ignore')
    df = df.dropna()

    cat_cols = ["waterfront", "view", "condition", "grade"]
    cols_possiveis = df.columns.tolist()
    if "price" in cols_possiveis:
        cols_possiveis.remove("price")

    # Fallback para vari√°veis padr√£o
    default_vars = ["sqft_living", "bathrooms"] + cat_cols
    default_vars = [v for v in default_vars if v in cols_possiveis]
    if not default_vars:
        default_vars = df.select_dtypes(include='number').columns.drop("price").tolist()[:2]

    st.markdown("### üéØ Sele√ß√£o de Vari√°veis Preditivas (Antes do One-Hot)")
    selected_features_raw = st.multiselect(
        "Selecione as vari√°veis para o modelo:",
        cols_possiveis,
        default=default_vars,
        key="seletor_variaveis_q1_etapa2"
    )

    # Separar vari√°veis num√©ricas e categ√≥ricas selecionadas
    selected_cat = [col for col in selected_features_raw if col in cat_cols]
    selected_num = [col for col in selected_features_raw if col not in cat_cols]

    if selected_features_raw:
        # Construir X com concatena√ß√£o segura
        frames = []
        if selected_num:
            frames.append(df[selected_num])
        if selected_cat:
            frames.append(pd.get_dummies(df[selected_cat], drop_first=True))

        if not frames:
            st.warning("‚ö†Ô∏è Nenhuma vari√°vel v√°lida foi selecionada.")
            return

        X = pd.concat(frames, axis=1)
        y = df["price"]

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        modelo = LinearRegression()
        modelo.fit(X_train, y_train)
        y_pred = modelo.predict(X_test)

        st.session_state["X_test"] = X_test
        st.session_state["y_test"] = y_test
        st.session_state["y_pred"] = y_pred

        st.markdown("### üìå Coeficientes do Modelo")
        coef_df = pd.DataFrame({"Vari√°vel": X.columns, "Coeficiente": modelo.coef_})
        st.dataframe(coef_df)

        st.markdown("### üìä M√©tricas de Avalia√ß√£o - Regressao Linear Classica")
        st.write(f"R¬≤: {r2_score(y_test, y_pred):.4f}")
        st.write(f"MAE: {mean_absolute_error(y_test, y_pred):.2f}")
        st.write(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}")

def q1_etapa3():
    st.markdown("---")
    st.header("Q1 - 3Ô∏è‚É£ Interpreta√ß√£o dos Resultados")
    if "X_test" in st.session_state and "y_test" in st.session_state and "y_pred" in st.session_state:
        X = st.session_state["X_test"]
        y = st.session_state["y_test"]
        y_pred = st.session_state["y_pred"]
        avaliar_pressupostos(X, y, y_pred)
    else:
        st.warning("‚ö†Ô∏è Execute a Etapa 2 para gerar o modelo antes de interpretar os resultados.")



def q1_etapa4():
    st.markdown("---")
    st.header("4Ô∏è‚É£ Ajustes no Modelo")
    df = st.session_state.get("kc_df")
    if df is None:
        st.warning("‚ö†Ô∏è Os dados ainda n√£o foram carregados. Execute a Etapa 1 primeiro.")
        st.stop()

    # Preprocessamento
    df = df.drop(columns=["id", "date", "zipcode"], errors='ignore')
    df = df[df["price"] > 0].dropna()

    st.markdown("### üîß Aplicando Transforma√ß√£o Logar√≠tmica em 'price'")
    df["log_price"] = np.log(df["price"])

    # Define vari√°veis categ√≥ricas para poss√≠veis sele√ß√µes
    cat_cols = ["waterfront", "view", "condition", "grade"]
    df[cat_cols] = df[cat_cols].astype("category")

    # Cria lista de colunas poss√≠veis (incluindo categ√≥ricas)
    all_cols = df.select_dtypes(include=["number", "category"]).columns.tolist()
    all_cols.remove("price")
    if "log_price" in all_cols:
        all_cols.remove("log_price")

    # Sugerir vari√°veis padr√£o
    default_vars = ["sqft_living", "bathrooms"] + cat_cols
    default_vars = [v for v in default_vars if v in all_cols]

    selected_features_raw = st.multiselect(
        "Selecione vari√°veis preditoras (num√©ricas ou categ√≥ricas):",
        all_cols,
        default=default_vars,
        key="seletor_variaveis_q1_etapa4"
    )

    # Separa entre categ√≥ricas e num√©ricas
    selected_cat = [col for col in selected_features_raw if col in cat_cols]
    selected_num = [col for col in selected_features_raw if col not in cat_cols]

    if selected_features_raw:
        # Aplica one-hot apenas nas categ√≥ricas selecionadas
        X_parts = []
        if selected_num:
            X_parts.append(df[selected_num])
        if selected_cat:
            X_parts.append(pd.get_dummies(df[selected_cat], drop_first=True))

        if not X_parts:
            st.warning("‚ö†Ô∏è Nenhuma vari√°vel v√°lida foi selecionada.")
            return

        X = pd.concat(X_parts, axis=1)
        y = df["log_price"]

        # Divis√£o e modelo
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        modelo = LinearRegression()
        modelo.fit(X_train, y_train)
        y_pred = modelo.predict(X_test)

        # Coeficientes
        st.markdown("### üìå Coeficientes (log_price)")
        coef_df = pd.DataFrame({"Vari√°vel": X.columns, "Coeficiente": modelo.coef_})
        st.dataframe(coef_df)

        # M√©tricas
        st.markdown("### üìä M√©tricas de Avalia√ß√£o - Regress√£o Linear ap√≥s transforma√ß√£o Log")
        st.write(f"R¬≤: {r2_score(y_test, y_pred):.4f}")
        st.write(f"MAE: {mean_absolute_error(y_test, y_pred):.2f}")
        st.write(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}")

        # Avalia√ß√£o de pressupostos
        avaliar_pressupostos(X_test, y_test, y_pred)

        # Lasso
        modelo_lasso = ajustar_modelo_lasso(X_train, y_train)
        y_pred_lasso = modelo_lasso.predict(X_test)

        st.markdown("### üìä M√©tricas de Avalia√ß√£o- Transforma√ß√£o Lasso")
        st.write(f"R¬≤: {r2_score(y_test, y_pred_lasso):.4f}")
        st.write(f"MAE: {mean_absolute_error(y_test, y_pred_lasso):.2f}")
        st.write(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_lasso)):.2f}")

        avaliar_pressupostos(X_test, y_test, y_pred_lasso)


def q1_etapa5():
    st.header("Q1 - 5Ô∏è‚É£ Tomada de Decis√£o")
    st.info("Descreva aplica√ß√µes pr√°ticas do modelo no contexto de neg√≥cio.")
    
    st.markdown("## üìå An√°lise da Quest√£o 1 ‚Äì Regress√£o Linear")

    st.markdown("""
    A an√°lise da Quest√£o 1 teve como objetivo prever os pre√ßos de im√≥veis na regi√£o de **King County** utilizando **Regress√£o Linear**.  
    A primeira etapa consistiu em uma **explora√ß√£o descritiva dos dados**, onde foram apresentadas estat√≠sticas como m√©dia, mediana e desvio padr√£o, al√©m de gr√°ficos de distribui√ß√£o e correla√ß√£o.  
    Essa an√°lise inicial ajudou a identificar vari√°veis mais relevantes para a modelagem, como `sqft_living`, `bathrooms` e `grade`.  
    Em seguida, foi constru√≠do um **modelo de regress√£o linear**, avaliado pelas m√©tricas:

    - **R¬≤**: `0.6084`
    - **MAE**: `R$ 155.744,00`
    - **RMSE**: `R$ 243.314,83`

    O desempenho inicial indicou uma **capacidade explicativa moderada**, mas com elevada variabilidade nos erros, sugerindo que o modelo cl√°ssico n√£o era suficientemente preciso.
    """)

    st.markdown("""
    Na etapa de interpreta√ß√£o, foram verificados os **pressupostos da regress√£o linear**, como:

    - Normalidade dos res√≠duos  
    - Homocedasticidade  
    - Aus√™ncia de multicolinearidade

    Todos os testes indicaram viola√ß√µes desses pressupostos.  
    Para tentar corrigir, foi aplicada a **transforma√ß√£o logar√≠tmica** na vari√°vel resposta `price`, resultando em um modelo com:

    - **R¬≤**: `0.6057`
    - **MAE**: `0.27`
    - **RMSE**: `0.34` (escala logar√≠tmica)

    Embora os res√≠duos t√™m se tornado mais sim√©tricos, os pressupostos **continuaram violados**.  
    Na sequ√™ncia, aplicou-se o modelo **Lasso com valida√ß√£o cruzada**, visando penalizar vari√°veis menos relevantes.  
    O desempenho do modelo Lasso foi:

    - **R¬≤**: `0.4882`
    - **MAE**: `0.31`
    - **RMSE**: `0.38`

    O Lasso teve desempenho inferior ao modelo log-transformado e **n√£o resolveu as viola√ß√µes estat√≠sticas**.
    """)

    st.markdown("""
    ### üíº Aplica√ß√£o no Neg√≥cio

    Mesmo com limita√ß√µes, os modelos desenvolvidos oferecem **insights √∫teis para neg√≥cios**.  
    Eles podem auxiliar:

    - Na **precifica√ß√£o inicial** de im√≥veis
    - Na **identifica√ß√£o de im√≥veis fora do padr√£o**
    - Na orienta√ß√£o de **investimentos em reformas**, destacando o impacto de vari√°veis como `grade` e `sqft_living`.

    Contudo, **devido √†s viola√ß√µes dos pressupostos**, recomenda-se o uso de **modelos n√£o lineares**, como:

    - √Årvores de Decis√£o  
    - Random Forest  
    - XGBoost

    Esses algoritmos lidam melhor com dados complexos e podem gerar **previs√µes mais confi√°veis e robustas** para o mercado imobili√°rio.
    """)

        

# =============================
# üîß Fun√ß√µes - Quest√£o 2
# =============================

def criar_coluna_arrival_date(df):
    """
    Adiciona uma coluna 'arrival_date' ao DataFrame com a data de chegada em formato datetime,
    convertendo o m√™s (por extenso) para n√∫mero.
    """
    # Dicion√°rio para convers√£o de nome do m√™s para n√∫mero
    meses = {
        'January': 1, 'February': 2, 'March': 3, 'April': 4,
        'May': 5, 'June': 6, 'July': 7, 'August': 8,
        'September': 9, 'October': 10, 'November': 11, 'December': 12
    }
    # Converter os nomes dos meses para n√∫mero
    df = df.copy()
    df['arrival_month_num'] = df['arrival_date_month'].map(meses)
    
    # Criar a coluna de data de chegada
   # df['arrival_date'] = pd.to_datetime(
   #     df['arrival_date_year'].astype(str) + '-' +
   #     df['arrival_month_num'].astype(str).str.zfill(2) + '-' +
   #     df['arrival_date_day_of_month'].astype(str).str.zfill(2)
   # )
    
    #st.write(df["arrival_date"])
    
     # Cria coluna booking_date (data da reserva)
    #df['booking_date'] = df['arrival_date'] - pd.to_timedelta(df['lead_time'], unit='D')
    #df.loc[:, 'booking_date'] = df['arrival_date'] - pd.to_timedelta(df['lead_time'], unit='D')
    
     #excluir arrival_date_year, arrival_date_month, arrival_date_week_number, arrival_date_day_of_month
    #variaves foram substitu√≠das por arrival_date

    #df = df.drop(columns=["arrival_date_year", "arrival_date_month", "arrival_date_week_number", "arrival_date_day_of_month"], errors='ignore')

    return df

def pre_processamento(df):
    """
    Realiza pr√©-processamento b√°sico dos dados
    """
    # Remove duplicatas
    df = df.drop_duplicates()
    #df = criar_coluna_arrival_date(df)
    #df["arrival_date"] = pd.to_datetime(df["arrival_date"], errors='coerce')
    #df['arrival_date'] = pd.to_datetime(df['arrival_date']).dt.round('ms')  # ou 's' para segundos
    #df['booking_date'] = pd.to_datetime(df['booking_date'], errors='coerce')
    
    return df



def q2_etapa1():
    st.header("Q2 - a) An√°lise Descritiva dos Dados")
    st.info("Realize uma an√°lise descritiva da base de dados.")
    uploaded_file = 'src/hotel_bookings.csv'
    # Carregar os dados

    df2 = carregar_dados(uploaded_file)
    st.session_state["hb_df"] = df2
    #st.success("‚úÖ Arquivo carregado com sucesso!")
    st.markdown("### Preview dos Dados")
    st.dataframe(df2.head())
    
 
    # Definindo os dados da tabela
    dados = {
        "Vari√°vel": [
            "hotel", "is_canceled", "lead_time", "arrival_date_year", "arrival_date_month",
            "arrival_date_week_number", "arrival_date_day_of_month", "stays_in_weekend_nights",
            "stays_in_week_nights", "adults", "children", "babies", "meal", "country",
            "market_segment", "distribution_channel", "is_repeated_guest", "previous_cancellations",
            "previous_bookings_not_canceled", "reserved_room_type", "assigned_room_type",
            "booking_changes", "deposit_type", "agent", "company", "days_in_waiting_list",
            "customer_type", "adr", "required_car_parking_spaces", "total_of_special_requests",
            "reservation_status", "reservation_status_date", "booking_date"
        ],
        "Tipo": [
            "Categ√≥rica", "Bin√°ria", "Num√©rica", "Num√©rica (inteira)", "Categ√≥rica",
            "Num√©rica (inteira)", "Num√©rica (inteira)", "Num√©rica (inteira)", "Num√©rica (inteira)",
            "Num√©rica (inteira)", "Num√©rica (inteira)", "Num√©rica (inteira)", "Categ√≥rica",
            "Categ√≥rica", "Categ√≥rica", "Categ√≥rica", "Bin√°ria", "Num√©rica (inteira)",
            "Num√©rica (inteira)", "Categ√≥rica", "Categ√≥rica", "Num√©rica (inteira)", "Categ√≥rica",
            "Categ√≥rica", "Categ√≥rica", "Num√©rica (inteira)", "Categ√≥rica", "Num√©rica (float)",
            "Num√©rica (inteira)", "Num√©rica (inteira)", "Categ√≥rica", "Data","Data"
        ],
        "Descri√ß√£o": [
            "Tipo de hotel (ex: Resort, City)",
            "Cancelamento da reserva (1 = sim, 0 = n√£o)",
            "Tempo de anteced√™ncia da reserva (dias)",
            "Ano de chegada",
            "M√™s de chegada",
            "N√∫mero da semana de chegada (1 a 52/53)",
            "Dia do m√™s de chegada",
            "Noites de fim de semana na reserva",
            "Noites de semana na reserva",
            "N√∫mero de adultos",
            "N√∫mero de crian√ßas",
            "N√∫mero de beb√™s",
            "Tipo de refei√ß√£o inclu√≠da",
            "Pa√≠s de origem do h√≥spede",
            "Segmento de mercado",
            "Canal de distribui√ß√£o",
            "H√≥spede recorrente (1 = sim, 0 = n√£o)",
            "N√∫mero de cancelamentos anteriores",
            "Reservas anteriores n√£o canceladas",
            "Tipo de quarto reservado",
            "Tipo de quarto atribu√≠do",
            "Altera√ß√µes realizadas na reserva",
            "Tipo de dep√≥sito",
            "Agente de reservas",
            "Empresa associada √† reserva",
            "Dias na lista de espera",
            "Tipo de cliente",
            "Di√°ria m√©dia (Average Daily Rate)",
            "Espa√ßos de estacionamento necess√°rios",
            "Total de pedidos especiais",
            "Status final da reserva (Check-Out, Canceled, No-Show)",
            "Data do status final da reserva",
            "Data da reserva"
        ]
    }

    # Criando o DataFrame
    df_variaveis = pd.DataFrame(dados)

    # Exibindo no Streamlit
    st.write("### Dicion√°rio de Vari√°veis do Dataset Hotel Booking Demand")
    st.dataframe(df_variaveis, use_container_width=True)

    # pre processamento
    df2 = pre_processamento(df2)
    
    
    st.session_state["hb_df"] = df2
    st.success("‚úÖ Pr√©-processamento conclu√≠do - Excluindo duplicadas se houver")
    #st.markdown("### üîç Preview dos Dados apos Inclusao da coluna data arrival_date e exclusao das colunas de data")
    #st.dataframe(df2.head())
    
    st.markdown("### üìä Estat√≠sticas Descritivas")
    st.subheader("Resumo Estat√≠stico")
    st.dataframe(df2.describe())
 
     # Limpar colunas irrelevantes
    df2 = df2.drop(columns=['RowNumber', 'CustomerId', 'Surname'], errors='ignore')
    
    
    # graficos de distribui√ß√£o
    cols_numericas = [
        'lead_time', 'stays_in_weekend_nights', 'stays_in_week_nights', 'adults', 'children',
        'babies', 'previous_cancellations', 'previous_bookings_not_canceled',
        'booking_changes', 'days_in_waiting_list', 'adr',
        'required_car_parking_spaces', 'total_of_special_requests'
    ]
   

    
     # Mapear cancelamentos para facilitar leitura nos gr√°ficos
    df2['cancelamento'] = df2['is_canceled'].map({0: 'N√£o Cancelada', 1: 'Cancelada'})
    
    

    # Calcular contagem e porcentagem
    contagem = df2['cancelamento'].value_counts()
    porcentagem = df2['cancelamento'].value_counts(normalize=True) * 100

    # Criar tabela
    tabela = pd.DataFrame({
        'Status da Reserva': contagem.index,
        'Quantidade': contagem.values,
        'Porcentagem (%)': porcentagem.values.round(2)
    })

    # Exibir resultado
    st.subheader("Quantidade e Porcentagem de Cancelamentos")
    st.dataframe(tabela)
    
    

    st.subheader("Distribui√ß√£o das Reservas (Canceladas vs. N√£o Canceladas)")
    fig1, ax1 = plt.subplots()
    sns.countplot(data=df2, x='cancelamento', ax=ax1)
    ax1.set_title("Distribui√ß√£o de Cancelamentos")
    st.pyplot(fig1)

    st.subheader("Tipo de Hotel vs Cancelamento")
    fig2, ax2 = plt.subplots()
    sns.countplot(data=df2, x='hotel', hue='cancelamento', ax=ax2)
    ax2.set_title("Cancelamentos por Tipo de Hotel")
    st.pyplot(fig2)

    st.subheader("Tempo de Anteced√™ncia (Lead Time) por Cancelamento")
    fig3, ax3 = plt.subplots()
    sns.boxplot(data=df2, x='cancelamento', y='lead_time', ax=ax3)
    ax3.set_title("Lead Time por Status de Cancelamento")
    st.pyplot(fig3)


    st.subheader("Distribui√ß√£o de Reservas por M√™s")
    fig4, ax4 = plt.subplots(figsize=(10, 4))
    order_months = ['January', 'February', 'March', 'April', 'May', 'June',
                    'July', 'August', 'September', 'October', 'November', 'December']
    sns.countplot(data=df2, x='arrival_date_month', order=order_months, hue='cancelamento', ax=ax4)
    ax4.set_title("Reservas por M√™s (com Cancelamentos)")
    plt.xticks(rotation=45)
    st.pyplot(fig4)

    st.subheader("Considera√ß√µes Finais da An√°lise Descritiva")
    st.markdown(r"""
    Com base nos gr√°ficos apresentados, observa-se que aproximadamente 27,49% das reservas foram canceladas, enquanto 72,51% foram efetivadas, o que revela uma taxa de cancelamento relevante no conjunto de dados. A an√°lise por tipo de hotel mostra que o City Hotel apresenta maior volume absoluto de cancelamentos em compara√ß√£o ao Resort Hotel, refletindo tamb√©m sua maior participa√ß√£o no total de reservas. Al√©m disso, h√° uma varia√ß√£o expressiva ao longo do ano: os meses de julho e agosto concentram o maior volume de reservas e de cancelamentos, sugerindo que a alta temporada influencia diretamente o comportamento de churn.

    Outro fator importante identificado √© o tempo de anteced√™ncia da reserva (lead time). As reservas que foram canceladas apresentam, em m√©dia, um lead time maior que as reservas mantidas, o que indica que clientes que reservam com muita anteced√™ncia t√™m maior probabilidade de cancelar. Esse insight pode ser utilizado para definir pol√≠ticas comerciais mais eficientes, como tarifas n√£o reembols√°veis, programas de fideliza√ß√£o ou a√ß√µes promocionais segmentadas, contribuindo para a redu√ß√£o da taxa de cancelamentos e para o aumento da previsibilidade operacional.
    """)


def q2_etapa2():
    
    from sklearn.pipeline import Pipeline
    from sklearn.compose import ColumnTransformer
    from sklearn.impute import SimpleImputer
    from sklearn.preprocessing import StandardScaler, OneHotEncoder
    from sklearn.linear_model import LogisticRegression
       # graficar a distribui√ß√£o da vari√°vel dependente
    import plotly.express as px
    # importar pandas como pd
    import pandas as pd
    import streamlit as st
    import altair as alt
    
    
    st.header("Q2 - b) Modelo de Regress√£o Log√≠stica")
    st.info("Construa o modelo de regress√£o log√≠stica e avalie seu desempenho.")

    df2 = st.session_state.get("hb_df")
    if df2 is None:
        st.warning("‚ö†Ô∏è Os dados ainda n√£o foram carregados. Execute a Etapa 1 primeiro.")
        return
    
    y = df2['is_canceled']
    X = df2.drop(columns='is_canceled')

    
        # Separe as features do tipo categ√≥rica e num√©rica
    cols_numericas = [
        'lead_time', 'stays_in_weekend_nights', 'stays_in_week_nights', 'adults', 'children',
        'babies', 'previous_cancellations', 'previous_bookings_not_canceled',
        'booking_changes', 'days_in_waiting_list', 'adr',
        'required_car_parking_spaces', 'total_of_special_requests'
    ]

    cols_categoricas = [
        'hotel', 'meal', 
        #'country',
        'market_segment', 'distribution_channel',
        'is_repeated_guest', 
        #'reserved_room_type', 'assigned_room_type',
        'deposit_type', 
        #'agent',
       # 'company', 
        'customer_type'
    ]
    

    # 1. Balanceamento das classes
    st.markdown("**Distribui√ß√£o da vari√°vel dependente (is_canceled):**")
    st.dataframe(y.value_counts(normalize=True).rename("Propor√ß√£o").to_frame())

    fig = px.pie(y.value_counts(normalize=True).rename("Propor√ß√£o").to_frame(), 
                 values='Propor√ß√£o', 
                 names=y.value_counts().index, 
                 title="Distribui√ß√£o de Cancelamentos (is_canceled)",
                 color_discrete_sequence=px.colors.qualitative.Set2)    
    st.plotly_chart(fig, use_container_width=True)
    

   
   # se distribui√ß√£o for desbalanceada, aplicar oversampling ou undersampling
    from imblearn.over_sampling import RandomOverSampler
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.pipeline import Pipeline as ImbPipeline
    from collections import Counter
    # Verificar o balanceamento das classes
    
   
    counter = Counter(y)
    #st.write(f"Classes antes do balanceamento: {counter}")
    # Se a distribui√ß√£o for desbalanceada, aplicar oversampling ou undersampling

    if counter[0] < counter[1]:
        st.warning("‚ö†Ô∏è Distribui√ß√£o desbalanceada. Aplicando oversampling para balancear as classes.")
        
        
        # Aplicar oversampling
        oversample = RandomOverSampler(sampling_strategy='minority', random_state=42)
        X_resampled, y_resampled = oversample.fit_resample(df2[cols_numericas + cols_categoricas], y)
        st.write(f"Classes ap√≥s oversampling: {Counter(y_resampled)}")
    elif counter[0] > counter[1]:
        st.warning("‚ö†Ô∏è Distribui√ß√£o desbalanceada. Aplicando undersampling para balancear as classes.")
        # Aplicar undersampling
        undersample = RandomUnderSampler(sampling_strategy='majority', random_state=42)
        X_resampled, y_resampled = undersample.fit_resample(df2[cols_numericas + cols_categoricas], y)
       # st.write(f"Classes ap√≥s undersampling: {Counter(y_resampled)}")
       
            # 3. Verifica√ß√£o de valores ausentes
        # 3. Verifica√ß√£o de valores ausentes no dataset balanceado
        st.markdown("**Valores ausentes nas colunas do dataset balanceado:**")
        missing_values = X.isnull().sum()
        missing_values = missing_values[missing_values > 0].sort_values(ascending=False)
        if missing_values.empty:
            st.success("‚úÖ N√£o h√° valores ausentes nas colunas do dataset balanceado.")
        else:
            st.dataframe(missing_values.rename("Valores Ausentes").to_frame())
            st.markdown("""
            Optou-se pela remo√ß√£o das vari√°veis `country`, `agent` e `company` do modelo devido √† alta propor√ß√£o 
            de valores ausentes e √† elevada cardinalidade dessas vari√°veis, que dificultariam o processamento e aumentariam 
            o risco de overfitting. Em contrapartida, a vari√°vel `hotel` foi mantida por sintetizar informa√ß√µes relevantes
            sobre o tipo de hospedagem e o perfil do h√≥spede, sendo capaz de capturar parte da variabilidade representada 
            pelas vari√°veis exclu√≠das. Dessa forma, o modelo permanece mais simples, robusto e eficiente, sem perda significativa de informa√ß√£o.
            """)

   
    else:
        st.success("‚úÖ Distribui√ß√£o balanceada. N√£o √© necess√°rio aplicar oversampling ou undersampling.")
        X_resampled = df2[cols_numericas + cols_categoricas]
        y_resampled = y
   
    
    # Exibir a distribui√ß√£o das classes ap√≥s o balanceamento
    st.markdown("**Distribui√ß√£o da vari√°vel dependente (is_canceled) ap√≥s balanceamento:**")
    st.dataframe(y_resampled.value_counts(normalize=True).rename("Propor√ß√£o").to_frame())
    # graficar a distribui√ß√£o da vari√°vel dependente ap√≥s balanceamento
    fig = px.pie(y_resampled.value_counts(normalize=True).rename("Propor√ß√£o").to_frame(),
                 values='Propor√ß√£o', 
                 names=y_resampled.value_counts().index, 
                 title="Distribui√ß√£o de Cancelamentos (is_canceled) ap√≥s Balanceamento",
                 color_discrete_sequence=px.colors.qualitative.Set2)
    st.plotly_chart(fig, use_container_width=True)

   # 2. Multicolinearidade (VIF) - Exemplo para as features num√©ricas
  # Imputa√ß√£o dos valores ausentes nas vari√°veis num√©ricas do dataset balanceado
    X_num_vif = pd.DataFrame(X_resampled[cols_numericas], columns=cols_numericas)
    X_num_vif = X_num_vif.fillna(X_num_vif.mean())

    # Calcular VIF
    from statsmodels.stats.outliers_influence import variance_inflation_factor

    vif_data = pd.DataFrame()
    vif_data["Feature"] = X_num_vif.columns
    vif_data["VIF"] = [variance_inflation_factor(X_num_vif.values, i) for i in range(X_num_vif.shape[1])]
    st.markdown("**Fatores de Infla√ß√£o de Vari√¢ncia (VIF):**")
    st.dataframe(vif_data)

    # validar se o VIF √© maior 10
    vif_threshold = 10
    if any(vif_data["VIF"] > vif_threshold):
        st.warning(f"‚ö†Ô∏è Algumas vari√°veis t√™m VIF maior que {vif_threshold}. Considere remover ou combinar vari√°veis.")
    else:
        st.success(f"‚úÖ Todas as vari√°veis t√™m VIF abaixo de {vif_threshold}. N√£o h√° multicolinearidade significativa.")


    
  
 
 
    # verificar linearidade entre as vari√°veis num√©ricas
    num_vars = [
        'lead_time'
    ]

    st.markdown("### An√°lise de Linearidade com Boxplot das Vari√°veis Num√©ricas por Cancelamento")

    import altair as alt
    for col in num_vars:
        chart = alt.Chart(df2).mark_boxplot(extent='min-max').encode(
            x=alt.X('is_canceled:N', title='Cancelamento (0 = N√£o, 1 = Sim)'),
            y=alt.Y(f'{col}:Q', title=col),
            color=alt.Color('is_canceled:N', legend=None)
        ).properties(
            title=f'{col} vs Cancelamento',
            width=400,
            height=300
        )
        st.altair_chart(chart, use_container_width=True)
  
    st.markdown("""
    Boxplots foram utilizados para comparar a distribui√ß√£o das vari√°veis num√©ricas entre as classes da vari√°vel dependente 
    (`is_canceled`). Observou-se que algumas vari√°veis, como `lead_time`, apresentaram diferen√ßas nas medianas entre as classes,
    sugerindo rela√ß√£o monot√¥nica desej√°vel para a regress√£o log√≠stica. Para as demais vari√°veis num√©ricas, essa tend√™ncia n√£o foi
    t√£o evidente, indicando que a rela√ß√£o linear com o logit pode ser mais fraca ou inexistente nessas vari√°veis. Ainda assim, 
    todas as vari√°veis foram mantidas no modelo, reconhecendo que sua contribui√ß√£o, mesmo que n√£o estritamente linear, pode ser
    relevante para a previs√£o. Recomenda-se, em aplica√ß√µes futuras, avaliar transforma√ß√µes ou m√©todos mais flex√≠veis caso o ajuste 
    linear n√£o seja suficiente.
    """)
    st.success("‚úÖ Pressupostos iniciais validados com sucesso! (Linearidade, Multicolinearidade, Balanceamento) Pronto para treinar o modelo de Regress√£o Log√≠stica.")

    # Pr√©-processamento
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ])
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, cols_numericas),
            ('cat', categorical_transformer, cols_categoricas)
        ]
    )

    pipeline = Pipeline(steps=[
        ('prep', preprocessor),
        ('logreg', LogisticRegression(max_iter=1000))
    ])


    # Split
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)

    # Treinamento
    #pipeline.fit(X_train, y_train)
    
    # Treinamento
    pipeline.fit(X_train, y_train)

    # Previs√µes
    y_pred = pipeline.predict(X_test)
    

    import numpy as np
    import pandas as pd

    # Obter nomes das features do pipeline para tabela de coeficientes
    feature_names = pipeline.named_steps['prep'].get_feature_names_out()
    coef = pipeline.named_steps['logreg'].coef_[0]
    odds = np.exp(coef)

    coef_table = pd.DataFrame({
        'Vari√°vel': feature_names,
        'Coeficiente': coef.round(3),
        'Odds Ratio': odds.round(3),
        'Interpreta√ß√£o': [
            "Aumenta chance de cancelamento" if c > 0 else "Reduz chance de cancelamento"
            for c in coef
        ]
    }).sort_values(by='Coeficiente', ascending=False)
        
    st.markdown("### üìä Coeficientes do Modelo de Regress√£o Log√≠stica")
    st.dataframe(coef_table) 
    # Salve o pipeline e dados de teste no session_state
    st.session_state["model"] = pipeline
   
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

    # Calcular m√©tricas
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    conf_mat = confusion_matrix(y_test, y_pred)
    report = classification_report(y_test, y_pred)

    # Exibir resultados no Streamlit
    st.markdown(f"""
    **Acur√°cia:** {acc:.3f}  
    **Precis√£o:** {prec:.3f}  
    **Recall:** {rec:.3f}  
    **F1-score:** {f1:.3f}  
    """)

    st.markdown("**Matriz de Confus√£o:**")
    st.dataframe(conf_mat)

    st.markdown("**Relat√≥rio de Classifica√ß√£o:**")
    st.text(report)
    
    
    # Tabela de m√©tricas
    metricas = pd.DataFrame({
        "M√©trica": ["Acur√°cia", "Precis√£o", "Recall", "F1-score"],
        "Valor": [0.721, 0.708, 0.754, 0.730],
        "Interpreta√ß√£o": [
            "O modelo acertou 72,1% das previs√µes totais (canceladas e n√£o canceladas).",
            "Das reservas que o modelo previu como canceladas, 70,8% realmente foram.",
            "O modelo identificou corretamente 75,4% das reservas que foram canceladas.",
            "M√©dia harm√¥nica entre precis√£o e recall, indicando bom equil√≠brio."
        ]
    })
    
    st.subheader("Explica√ß√£o das M√©tricas do Modelo de Regress√£o Log√≠stica")
    st.dataframe(metricas)

     
     
def q2_etapa3():
        st.markdown("---")
        st.header("Q2 - c) An√°lise das Features")
        st.info("Analise a import√¢ncia das vari√°veis no modelo de regress√£o log√≠stica.")

        df2 = st.session_state.get("hb_df")
        if df2 is None:
            st.warning("‚ö†Ô∏è Os dados ainda n√£o foram carregados. Execute a Etapa 1 primeiro.")
            return
        
        
        if "model" not in st.session_state:
            st.warning("‚ö†Ô∏è Execute a Etapa 2 para treinar o modelo antes de analisar as features.")
            return

        model = st.session_state["model"]
        #X_test = st.session_state["X_test"]

        feature_names = model.named_steps['prep'].get_feature_names_out()
        coef = model.named_steps['logreg'].coef_[0]

        print(len(feature_names), len(coef))  # Ambos devem ter o mesmo tamanho

        feature_importance = pd.DataFrame({
            'Feature': feature_names,
            'Importance': coef
        }).sort_values(by='Importance', ascending=False)

        st.markdown("### üìä Import√¢ncia das Vari√°veis")
        st.dataframe(feature_importance)

        # Gr√°fico de barras
        fig = px.bar(feature_importance, x='Feature', y='Importance', title='Import√¢ncia das Vari√°veis')
        st.plotly_chart(fig, use_container_width=True)

        st.subheader("An√°lise Final das Principais Vari√°veis")

        st.markdown("""
        ### üî∫ Cinco vari√°veis que mais **aumentam** a chance de cancelamento:

        1. **`deposit_type_Non Refund`**  
        Dep√≥sitos n√£o reembols√°veis est√£o associadas a uma **alta chance de cancelamento negativo** (impacto negativo forte no modelo), ou seja, sua aus√™ncia pode elevar o risco.

        2. **`required_car_parking_spaces`**  
        Quanto **menos necessidade de vaga de estacionamento**, maior a probabilidade de o cliente cancelar ‚Äî sugerindo menor comprometimento.

        3. **`market_segment_Offline TA/TO`**  
        Reservas feitas por ag√™ncias offline parecem estar mais ligadas a cancelamentos.

        4. **`deposit_type_Refundable`**  
        A op√ß√£o de **reembolso** facilita o cancelamento, aumentando sua probabilidade.

        5. **`market_segment_Groups`**  
        Embora grupos sejam geralmente mais est√°veis, nesse caso espec√≠fico os dados indicam **maior risco de cancelamento**, talvez por volume ou incerteza log√≠stica.

        ---

        ### üîª Cinco vari√°veis que mais **reduzem** a chance de cancelamento:

        1. **`lead_time`**  
        Reservas feitas com bastante anteced√™ncia est√£o **menos propensas a serem canceladas**, indicando planejamento.

        2. **`customer_type_Transient`**  
        H√≥spedes de tipo transit√≥rio demonstram **baixo risco de cancelamento**, talvez por viagens r√°pidas e com datas fixas.

        3. **`previous_cancellations`**  
        Curiosamente, clientes com hist√≥rico anterior **t√™m menor peso negativo aqui**, indicando que talvez tenham retornado com mais compromisso.

        4. **`market_segment_Complementary`**  
        Reservas promocionais ou gratuitas apresentam **menor risco de cancelamento**, talvez por serem incentivos vinculados a eventos espec√≠ficos.

        5. **`distribution_channel_Undefined`**  
        Quando o canal de distribui√ß√£o n√£o √© especificado, o modelo entende que h√° **menos risco de cancelamento**, possivelmente por padr√£o de preenchimento.

        ---

        Essas vari√°veis ajudam a identificar **perfis de clientes, canais e reservas** com maior ou menor probabilidade de cancelamento, permitindo **a√ß√µes preventivas** e estrat√©gias comerciais mais eficazes.
        """)


        
        
def q2_etapa4():
    st.markdown("---")
    st.header("Q2 - d) Justificativa do M√©todo")
    st.info("Discuta a escolha do modelo de regress√£o log√≠stica.")

    df2 = st.session_state.get("hb_df")
    if df2 is None:
        st.warning("‚ö†Ô∏è Os dados ainda n√£o foram carregados. Execute a Etapa 1 primeiro.")
        return

    if "model" not in st.session_state:
        st.warning("‚ö†Ô∏è Execute a Etapa 2 para treinar o modelo antes de justificar o m√©todo.")
        return

    model = st.session_state["model"]
 

    # Justificativa do m√©todo
    st.markdown("""
    A regress√£o log√≠stica foi escolhida por ser o m√©todo mais adequado para problemas de classifica√ß√£o bin√°ria, como √© o caso da previs√£o de cancelamento de reservas, cuja vari√°vel alvo assume apenas dois valores: cancelado (1) ou n√£o cancelado (0).

    Diferentemente da regress√£o linear, que prev√™ valores cont√≠nuos e pode gerar resultados fora do intervalo [0, 1], a regress√£o log√≠stica modela diretamente a probabilidade de ocorr√™ncia de um evento. Seu resultado est√° sempre restrito ao intervalo de 0 a 1, permitindo interpretar a sa√≠da do modelo como a probabilidade de cancelamento de cada reserva.

    Al√©m disso, a regress√£o log√≠stica lida melhor com a natureza categ√≥rica da vari√°vel resposta e permite calcular m√©tricas de avalia√ß√£o apropriadas, como acur√°cia, precis√£o, recall e F1-score, que s√£o fundamentais para problemas de classifica√ß√£o. Portanto, a regress√£o log√≠stica oferece maior robustez, interpretabilidade e adequa√ß√£o estat√≠stica para o contexto deste estudo, sendo prefer√≠vel √† regress√£o linear.
    """)



# =============================
# üîß Fun√ß√µes - Quest√£o 3
# =============================
def q3_etapa1():
    st.header("Q3 - a) An√°lise Descritiva")
    st.info("Explore dados por pa√≠s, quantidade e pre√ßo.")
        
        # Carregar a planilha (ou use df se j√° estiver carregado)
    @st.cache_data
    def carregar_dados():
        return pd.read_csv("src/planilha_combinada.csv")

    df = carregar_dados()
    
    #adicionar df na sess√£o do streamlit
    st.session_state["df3"] = df

    # Total de linhas
    total_linhas = len(df)

    # Calcular valores ausentes
    valores_ausentes = df.isnull().sum()
    percentual_ausentes = (valores_ausentes / total_linhas) * 100

    # Juntar em um √∫nico DataFrame para exibi√ß√£o
    tabela_ausentes = pd.DataFrame({
        'Valores Ausentes': valores_ausentes,
        'Percentual (%)': percentual_ausentes.round(2)
    })

    # Filtrar apenas colunas com pelo menos 1 valor ausente
    tabela_ausentes = tabela_ausentes[tabela_ausentes['Valores Ausentes'] > 0]

    # mostrar valores ausentes no streamlit
    st.markdown("### Valores Ausentes por Coluna")
    st.dataframe(tabela_ausentes.sort_values(by="Percentual (%)", ascending=False))
     
    # Exibir o DataFrame original
    st.markdown("### Preview dos Dados")
    st.dataframe(df.head())
    # Exibir estat√≠sticas descritivas
    st.markdown("### Estat√≠sticas Descritivas")
    st.dataframe(df.describe())
    

    # 4. Gr√°ficos de distribui√ß√£o
    st.subheader("üìà Distribui√ß√µes")

    # 6. Evolu√ß√£o temporal
    st.subheader("üìÖ Evolu√ß√£o Temporal das Vendas")
    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
    df['M√™s'] = df['InvoiceDate'].dt.to_period('M').astype(str)
    vendas_mes = df.groupby("M√™s")['Quantity'].sum().reset_index()
    st.line_chart(vendas_mes.set_index("M√™s"))
        
   
    # stockcod por pa√≠s
    vendas_pais_stockcode = df.groupby("Country")["StockCode"].nunique().reset_index()
    vendas_pais_stockcode = vendas_pais_stockcode.sort_values(by="StockCode", ascending=False)
    st.markdown("### üìä Quantidade de Produtos Vendidos por Pa√≠s")
    st.dataframe(vendas_pais_stockcode)
    
    # grafico stats.probplot de quantidade de produtos vendidos por pa√≠s
    # use probplot para verificar a distribui√ß√£o
    import scipy.stats as stats
    import matplotlib.pyplot as plt
    import seaborn as sns
   

    # Verifique se a coluna 'StockCode' √© num√©rica
    vendas_pais_stockcode["StockCode"] = pd.to_numeric(vendas_pais_stockcode["StockCode"], errors="coerce")

    # Cria√ß√£o da figura
    fig_stockcode = plt.figure(figsize=(10, 6))

    # Gr√°fico Q-Q plot (verifica√ß√£o de normalidade)
    stats.probplot(vendas_pais_stockcode["StockCode"].dropna(), dist="norm", plot=plt)
    plt.title("Distribui√ß√£o de Produtos Vendidos por Pa√≠s")

    # Renderiza√ß√£o no Streamlit
    st.pyplot(fig_stockcode)
    
    # Suponha que voc√™ j√° tenha o DataFrame `df`
    # Certifique-se de que 'Price' √© num√©rico
    df["Price"] = pd.to_numeric(df["Price"], errors="coerce")

    # Cria√ß√£o da figura
    fig_price = plt.figure(figsize=(10, 6))

    # Gr√°fico Q-Q plot para verificar normalidade do pre√ßo
    stats.probplot(df["Price"].dropna(), dist="norm", plot=plt)
    plt.title("Q-Q Plot - Distribui√ß√£o do Pre√ßo dos Produtos")

    # Exibir no Streamlit
    st.pyplot(fig_price)

    st.subheader("Resumo da An√°lise Descritiva ")
    
    st.markdown("""
    A an√°lise do Q-Q Plot para o **pre√ßo dos produtos** revela uma clara assimetria √† direita, indicando que a distribui√ß√£o dos valores n√£o √© normal. Observa-se a presen√ßa de diversos **outliers de alta magnitude**, com uma curvatura acentuada no gr√°fico e dispers√£o evidente dos pontos em rela√ß√£o √† linha te√≥rica de normalidade. Isso sugere uma elevada **variabilidade nos pre√ßos**, com alguns produtos muito caros distorcendo a m√©dia e influenciando fortemente as estat√≠sticas descritivas tradicionais.

    Da mesma forma, a **quantidade de produtos vendidos por pa√≠s** tamb√©m n√£o apresenta distribui√ß√£o normal. O gr√°fico evidencia uma forte **concentra√ß√£o de pa√≠ses com baixas vendas** e um pequeno n√∫mero de pa√≠ses com volumes expressivamente maiores. A linha te√≥rica de normalidade √© ultrapassada nos extremos, refor√ßando a ideia de uma **distribui√ß√£o assim√©trica com cauda pesada**, o que indica que testes estat√≠sticos n√£o param√©tricos s√£o mais adequados para esse tipo de dado.
    """)

            
def q3_etapa2():
    st.header("Q3 - b) ANOVA entre Pa√≠ses")
    st.info("Apresente F, p-valor e interprete o teste.")
        
    import pandas as pd
    import statsmodels.api as sm
    from statsmodels.formula.api import ols
   
    #recuperar df3 da sess√£o do streamlit
    df3 = st.session_state.get("df3")
    if df3 is None:
        st.warning("‚ö†Ô∏è Os dados ainda n√£o foram carregados. Execute a Etapa 1 primeiro.")
        return


    import statsmodels.api as sm
    import statsmodels.formula.api as smf
    import pandas as pd

    # Exemplo: carregando o DataFrame (substitua pelo seu)
    # df = pd.read_excel("planilha_combinada_amostrada.xlsx")

    st.subheader("ANOVA - Compara√ß√£o de M√©dias por Pa√≠s")

    # ANOVA para Quantity ~ Country
    modelo_q = smf.ols("Quantity ~ C(Country)", data=df3).fit()
    anova_q = sm.stats.anova_lm(modelo_q, typ=2)

    # ANOVA para Price ~ Country
    modelo_p = smf.ols("Price ~ C(Country)", data=df3).fit()
    anova_p = sm.stats.anova_lm(modelo_p, typ=2)

    # Resultado Quantity
    st.subheader("üîπ ANOVA: Quantity por Pa√≠s")
    st.dataframe(anova_q)

    f_q = anova_q.loc["C(Country)", "F"]
    p_q = anova_q.loc["C(Country)", "PR(>F)"]

    if p_q < 0.001:
        st.success(f"Resultado: influ√™ncia **muito significativa** (F = {f_q:.2f}, p < 0.001)")
    elif p_q < 0.05:
        st.info(f"Resultado: influ√™ncia **significativa** (F = {f_q:.2f}, p = {p_q:.4f})")
    else:
        st.warning(f"Resultado: **sem influ√™ncia significativa** (F = {f_q:.2f}, p = {p_q:.4f})")
        
   
        
    # Resultado Price
    st.subheader("üîπ ANOVA: Price por Pa√≠s")
    st.dataframe(anova_p)

    f_p = anova_p.loc["C(Country)", "F"]
    p_p = anova_p.loc["C(Country)", "PR(>F)"]

    if p_p < 0.001:
        st.success(f"Resultado: influ√™ncia **muito significativa** (F = {f_p:.2f}, p < 0.001)")
    elif p_p < 0.05:
        st.info(f"Resultado: influ√™ncia **significativa** (F = {f_p:.2f}, p = {p_p:.4f})")
    else:
        st.warning(f"Resultado: **sem influ√™ncia significativa** (F = {f_p:.2f}, p = {p_p:.4f})")
        
        
        
    st.markdown("""
    ### Interpreta√ß√£o do Resultado - Compara√ß√£o de M√©dias por Pa√≠s

    A an√°lise de vari√¢ncia (ANOVA) foi utilizada para verificar se as m√©dias de **quantidade de produtos vendidos** e de **pre√ßo dos produtos** diferem significativamente entre os pa√≠ses.

    üîπ **Quantity por Pa√≠s**  
    - Estat√≠stica F = **131.59**  
    - Valor-p = **< 0.001**  
    ‚û°Ô∏è **Resultado:** Evid√™ncia estat√≠stica muito forte de que a quantidade m√©dia de produtos vendidos varia significativamente entre os pa√≠ses. Isso indica que o pa√≠s de venda exerce uma **influ√™ncia substancial** nas quantidades comercializadas.

    üîπ **Price por Pa√≠s**  
    - Estat√≠stica F = **5.95**  
    - Valor-p = **< 0.001**  
    ‚û°Ô∏è **Resultado:** H√° uma **influ√™ncia estatisticamente significativa** do pa√≠s sobre os pre√ßos m√©dios praticados. Embora o efeito seja menos intenso que o observado para quantidade, ainda assim √© estatisticamente relevante.

    Esses resultados indicam que tanto o volume quanto o pre√ßo de produtos variam de forma significativa entre os pa√≠ses, refor√ßando a import√¢ncia de considerar o fator geogr√°fico em an√°lises comerciais e estrat√©gias de mercado.
    """)
       
    

def verificar_pressupostos_anova(df, var_resposta, fator_categ='Country'):
    """
    Verifica os pressupostos da ANOVA:
    - Normalidade dos res√≠duos (Shapiro-Wilk + Q-Q plot)
    - Homocedasticidade (Breusch-Pagan + gr√°fico de res√≠duos)

    Par√¢metros:
    - df: DataFrame com os dados
    - var_resposta: vari√°vel num√©rica dependente (ex: 'Quantity', 'Price')
    - fator_categ: vari√°vel categ√≥rica (ex: 'Country')

    Exibe resultados e interpreta√ß√µes no Streamlit.
    """
    import streamlit as st
    import statsmodels.formula.api as smf
    import statsmodels.api as sm
    from statsmodels.stats.diagnostic import het_breuschpagan
    from scipy.stats import shapiro, zscore, kstest
    import matplotlib.pyplot as plt
    import seaborn as sns

    st.subheader(f"Verifica√ß√£o dos Pressupostos da ANOVA - {var_resposta} ~ {fator_categ}")

    # Ajustar modelo
    formula = f"{var_resposta} ~ C({fator_categ})"
    modelo = smf.ols(formula, data=df).fit()
    residuos = modelo.resid
    valores_previstos = modelo.fittedvalues

    violou_normalidade = False
    violou_homocedasticidade = False            

    
    #  Normalidade dos res√≠duos
    st.subheader("1Ô∏è Normalidade dos Res√≠duos (Q-Q Plot + Kolmogorov-Smirnov)")

    # Criar um container estreito com colunas
    col1, col2, col3 = st.columns([1, 2, 1])  # centraliza

    with col1:
        fig, ax = plt.subplots(figsize=(3, 2.5))
        sm.qqplot(residuos, line='s', ax=ax)
        ax.set_title("Q-Q Plot dos Res√≠duos", fontsize=10)
        st.pyplot(fig)

    # Aplicar o teste Kolmogorov-Smirnov com res√≠duos padronizados
    residuos_padronizados = zscore(residuos)
    ks_stat, ks_p = kstest(residuos_padronizados, 'norm')

    if ks_p < 0.05:
        violou_normalidade = True
        st.warning("Essa vers√£o evita o alerta do Shapiro para N > 5000 e √© mais apropriada para grandes amostras.")
        st.warning(f"‚ùó Kolmogorov-Smirnov indica viola√ß√£o da normalidade (U)(p = {ks_p:.4f})")
    else:
        st.success(f"‚úÖ Res√≠duos seguem distribui√ß√£o normal (Kolmogorov-Smirnov p = {ks_p:.4f})")
   

    # Layout com colunas para centralizar e limitar largura
    #col1, col2, col3 = st.columns([1, 2, 1])  # col2 √© a central

    with col2:
        with st.container():
            fig2, ax2 = plt.subplots(figsize=(3, 2.5), dpi=100)
            sns.scatterplot(x=valores_previstos, y=residuos, s=10, ax=ax2)
            ax2.axhline(0, color='red', linestyle='--')
            ax2.set_xlabel("Valores Previstos", fontsize=8)
            ax2.set_ylabel("Res√≠duos", fontsize=8)
            ax2.set_title("Res√≠duos vs. Valores Previstos", fontsize=10)
            ax2.tick_params(axis='both', labelsize=7)
            fig2.tight_layout()
            st.pyplot(fig2, use_container_width=False)

    # Teste de Breusch-Pagan
    bp_test = het_breuschpagan(residuos, modelo.model.exog)
    p_bp = bp_test[3]  # p-valor do teste (stat, pval, fval, f_pval)

    st.write(f"üî¨ Breusch-Pagan (homocedasticidade dos res√≠duos): p = {p_bp:.4f}")
    if p_bp < 0.05:
        violou_homocedasticidade = True
        st.warning("‚ùó Viola√ß√£o da homocedasticidade detectada (p < 0.05)")
    else:
        st.success("‚úÖ Homocedasticidade verificada (p ‚â• 0.05)")

    # Diagn√≥stico final
    st.subheader("Diagn√≥stico Final dos Pressupostos")
    if violou_normalidade or violou_homocedasticidade:
        st.error("‚ö†Ô∏è Um ou mais pressupostos da ANOVA foram violados. Considere usar testes n√£o param√©tricos : teste de Kruskal-Wallis.")
    else:
        st.success("‚úÖ Todos os pressupostos foram atendidos. A ANOVA √© apropriada.")




def q3_etapa3():
    st.header("Q3 - c) Ajustes no Modelo")    
    st.info("Verifique normalidade, homocedasticidade   etc.")
    
    # recuperar df3 da sess√£o do streamlit
    df3 = st.session_state.get("df3")
    
    if df3 is None:
        st.warning("‚ö†Ô∏è Os dados ainda n√£o foram carregados. Execute a Etapa 1 primeiro.")
        return
    
    verificar_pressupostos_anova(df3, var_resposta="Quantity", fator_categ="Country")
    verificar_pressupostos_anova(df3, var_resposta="Price", fator_categ="Country")  
    
    
        # aplicar  Kruskal-Wallis
    from scipy.stats import kruskal

    st.subheader("Q3 - c) Teste Kruskal-Wallis")
    st.info("Teste n√£o param√©trico para comparar medianas entre grupos.")
    anchor = "teste_kruskal_wallis"
    st.markdown(f"<a id='{anchor}'></a>", unsafe_allow_html=True)

    # Agrupar os dados por pa√≠s
    grupos_quantity = [group["Quantity"].values for name, group in df3.groupby("Country")]
    grupos_price = [group["Price"].values for name, group in df3.groupby("Country")]

    # Aplicar o teste Kruskal-Wallis
    kruskal_quantity = kruskal(*grupos_quantity)
    kruskal_price = kruskal(*grupos_price)

    # Exibir os resultados
    st.markdown("### Teste Kruskal-Wallis - Compara√ß√£o de Medianas")
    st.write(f"üî¨ Kruskal-Wallis (Quantidade): H = {kruskal_quantity.statistic:.4f}, p = {kruskal_quantity.pvalue:.4f}")
    st.write(f"üî¨ Kruskal-Wallis (Pre√ßo): H = {kruskal_price.statistic:.4f}, p = {kruskal_price.pvalue:.4f}")

    # An√°lise autom√°tica dos resultados
    st.markdown("### An√°lise Comparativa")

    # Interpreta√ß√£o Quantity
    if kruskal_quantity.pvalue < 0.05:
        st.success("‚úÖ Diferen√ßa significativa detectada nas **quantidades vendidas entre pa√≠ses** (p < 0.05).")
    else:
        st.warning("‚ö†Ô∏è N√£o foi detectada diferen√ßa significativa nas **quantidades vendidas entre pa√≠ses** (p ‚â• 0.05).")

    # Interpreta√ß√£o Price
    if kruskal_price.pvalue < 0.05:
        st.success("‚úÖ Diferen√ßa significativa detectada nos **pre√ßos praticados entre pa√≠ses** (p < 0.05).")
    else:
        st.warning("‚ö†Ô∏è N√£o foi detectada diferen√ßa significativa nos **pre√ßos praticados entre pa√≠ses** (p ‚â• 0.05).")





def q3_etapa4():
    st.header("Q3 - d) Interpreta√ß√£o e Decis√£o")
    
    
    from scipy.stats import kruskal

    st.info(" Interpreta√ß√£o do Teste Kruskal-Wallis - Teste n√£o param√©trico para comparar medianas entre grupos, Decis√µes com base nas diferen√ßas de m√©dias")

    df3 = st.session_state.get("df3")
    if df3 is None:
        st.warning("‚ö†Ô∏è Os dados ainda n√£o foram carregados. Execute a Etapa 1 primeiro.")
        return

    grupos_quantity = [group["Quantity"].values for name, group in df3.groupby("Country")]
    grupos_price = [group["Price"].values for name, group in df3.groupby("Country")]

    kruskal_quantity = kruskal(*grupos_quantity)
    kruskal_price = kruskal(*grupos_price)

    st.markdown("### Teste Kruskal-Wallis - Compara√ß√£o de Medianas")
    st.write(f"üî¨ Kruskal-Wallis (Quantidade): H = {kruskal_quantity.statistic:.4f}, p = {kruskal_quantity.pvalue:.4f}")
    st.write(f"üî¨ Kruskal-Wallis (Pre√ßo): H = {kruskal_price.statistic:.4f}, p = {kruskal_price.pvalue:.4f}")

    st.markdown("""
    ### An√°lise dos Resultados

    Nesta etapa, foi aplicado o **teste de Kruskal-Wallis**, uma t√©cnica n√£o param√©trica utilizada para comparar as **medianas** de m√∫ltiplos grupos independentes, neste caso, os diferentes **pa√≠ses**.

    Os resultados apontam:

    - **Quantidade Vendida**: O valor da estat√≠stica de Kruskal-Wallis foi **H = {:.4f}**, com um **p-valor de {:.4f}**, indicando uma diferen√ßa estatisticamente significativa entre os pa√≠ses quanto √† quantidade de produtos vendidos.
    - **Pre√ßo dos Produtos**: A estat√≠stica H foi de **{:.4f}**, tamb√©m com **p-valor de {:.4f}**, evidenciando que os pre√ßos praticados tamb√©m variam significativamente entre os pa√≠ses.

    Ambos os resultados t√™m **p < 0,05**, o que leva √† rejei√ß√£o da hip√≥tese nula de igualdade das medianas entre os grupos. Isso confirma que **existem diferen√ßas significativas tanto nas quantidades quanto nos pre√ßos entre os pa√≠ses analisados**.

    Essas varia√ß√µes podem estar associadas a fatores econ√¥micos locais, estrat√©gias de mercado, pol√≠ticas comerciais ou mesmo particularidades culturais e regionais que afetam o consumo e o valor dos produtos. Portanto, **estrat√©gias de venda e precifica√ß√£o devem considerar essas diferen√ßas para maior assertividade e competitividade em cada mercado nacional**.
    """.format(
        kruskal_quantity.statistic, kruskal_quantity.pvalue,
        kruskal_price.statistic, kruskal_price.pvalue
    ))
    
    
    

# =============================
# üîß Fun√ß√µes - Quest√£o 4
# =============================
def q4_etapa1():
    st.header("Q4 - a) Discuss√£o do Problema")
    st.info("Import√¢ncia de prever reclama√ß√µes no varejo.")

def q4_etapa2():
    import streamlit as st
    import pandas as pd
    import numpy as np
    import seaborn as sns
    import matplotlib.pyplot as plt

    

    st.title("üìä An√°lise Descritiva: Customer Personality Analysis")

    # Carregar dados
    @st.cache_data
    def carregar_dados():
        return pd.read_csv("src/marketing_campaign.csv", sep="\t")

    df4 = carregar_dados()
    
    # iNserir df4 na sess√£o do streamlit
    st.session_state["df4"] = df4
    # Exibir o DataFrame original
    
    with st.expander("üìö Dicion√°rio de Dados"):
        st.markdown("""
    ### üë§ Pessoas
    - **ID**: Identificador exclusivo do cliente  
    - **Year_Birth**: Ano de nascimento do cliente  
    - **Education**: N√≠vel de escolaridade do cliente  
    - **Marital_Status**: Estado civil do cliente  
    - **Income**: Renda familiar anual do cliente  
    - **Kidhome**: N√∫mero de crian√ßas na casa do cliente  
    - **Teenhome**: N√∫mero de adolescentes na casa do cliente  
    - **Dt_Customer**: Data de inscri√ß√£o do cliente na empresa  
    - **Recency**: N√∫mero de dias desde a √∫ltima compra do cliente  
    - **Complain**: 1 se o cliente reclamou nos √∫ltimos 2 anos, 0 caso contr√°rio  

    ---

    ### üõçÔ∏è Produtos
    - **MntWines**: Valor gasto em vinho nos √∫ltimos 2 anos  
    - **MntFruits**: Valor gasto em frutas nos √∫ltimos 2 anos  
    - **MntMeatProducts**: Valor gasto em carne nos √∫ltimos 2 anos  
    - **MntFishProducts**: Valor gasto em peixes nos √∫ltimos 2 anos  
    - **MntSweetProducts**: Valor gasto em doces nos √∫ltimos 2 anos  
    - **MntGoldProds**: Valor gasto em ouro nos √∫ltimos 2 anos  

    ---

    ### üéØ Promo√ß√£o
    - **NumDealsPurchases**: N√∫mero de compras feitas com desconto  
    - **AcceptedCmp1**: 1 se o cliente aceitou a oferta na 1¬™ campanha  
    - **AcceptedCmp2**: 1 se o cliente aceitou a oferta na 2¬™ campanha  
    - **AcceptedCmp3**: 1 se o cliente aceitou a oferta na 3¬™ campanha  
    - **AcceptedCmp4**: 1 se o cliente aceitou a oferta na 4¬™ campanha  
    - **AcceptedCmp5**: 1 se o cliente aceitou a oferta na 5¬™ campanha  
    - **Response**: 1 se o cliente aceitou a oferta na √∫ltima campanha  

    ---

    ### üè¨ Lugar (Canal de Compra)
    - **NumWebPurchases**: N√∫mero de compras feitas atrav√©s do site  
    - **NumCatalogPurchases**: N√∫mero de compras feitas usando cat√°logo  
    - **NumStorePurchases**: N√∫mero de compras feitas diretamente nas lojas  
    - **NumWebVisitsMonth**: N√∫mero de visitas ao site no √∫ltimo m√™s  
    """)

    
    
    

    # ----------- Tratamento inicial ----------- #
    df4["Income"] = df4["Income"].fillna(df4["Income"].median())
    df4.drop(columns=["Z_CostContact", "Z_Revenue", "ID"], inplace=True)
    df4["Age"] = 2025 - df4["Year_Birth"]
    df4["Dt_Customer"] = pd.to_datetime(df4["Dt_Customer"], format="%d-%m-%Y")
    df4["TotalChildren"] = df4["Kidhome"] + df4["Teenhome"]
    df4["TotalSpent"] = df4[[
        "MntWines", "MntFruits", "MntMeatProducts", 
        "MntFishProducts", "MntSweetProducts", "MntGoldProds"
    ]].sum(axis=1)

    # Codifica√ß√£o
    df_encoded = pd.get_dummies(df4, columns=["Education", "Marital_Status"], drop_first=True)

    # ----------- Painel de An√°lise ----------- #
    st.markdown("## üîç Vis√£o Geral da Base")
    st.dataframe(df4.head())

    st.markdown("## Tratamento de Dados")
    st.write("Valores ausentes tratados. Renda preenchida com mediana.")
    st.write("Vari√°veis derivadas adicionadas: `Age`, `TotalChildren`, `TotalSpent`")

    # ----------- Estat√≠sticas e Segmentos ----------- #
    st.markdown("##  Estat√≠sticas Descritivas por Grupo")

    with st.expander("üë§ Pessoas"):
        st.write(df4[["Age", "Income", "Kidhome", "Teenhome", "TotalChildren"]].describe())
        fig1, ax1 = plt.subplots()
        sns.histplot(df4["Age"], bins=20, kde=True, ax=ax1)
        ax1.set_title("Distribui√ß√£o da Idade")
        st.pyplot(fig1)

    with st.expander("üõçÔ∏è Produtos"):
        cols_prod = ["MntWines", "MntFruits", "MntMeatProducts",
                    "MntFishProducts", "MntSweetProducts", "MntGoldProds", "TotalSpent"]
        st.write(df4[cols_prod].describe())
        fig2, ax2 = plt.subplots(figsize=(10, 4))
        sns.boxplot(data=df4[cols_prod], ax=ax2)
        ax2.set_title("Gasto por Categoria de Produto")
        ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45)
        st.pyplot(fig2)

    with st.expander("üéØ Promo√ß√µes"):
        cols_promo = ["AcceptedCmp1", "AcceptedCmp2", "AcceptedCmp3",
                    "AcceptedCmp4", "AcceptedCmp5", "Response", "NumDealsPurchases"]
        st.write(df4[cols_promo].sum().to_frame("Total de Aceites/Compras"))

    with st.expander("üè¨ Lugar / Canal"):
        cols_lugar = ["NumWebPurchases", "NumCatalogPurchases", 
                    "NumStorePurchases", "NumWebVisitsMonth"]
        st.write(df4[cols_lugar].describe())
        fig3, ax3 = plt.subplots()
        sns.heatmap(df4[cols_lugar].corr(), annot=True, cmap="coolwarm", ax=ax3)
        st.pyplot(fig3)

    # ----------- Rela√ß√£o com Reclamar ----------- #
    st.markdown("## üéØ Reclama√ß√µes (Vari√°vel Alvo)")

    st.write("Distribui√ß√£o de Reclama√ß√µes")
    fig4, ax4 = plt.subplots()
    sns.countplot(x="Complain", data=df4, ax=ax4)
    ax4.set_title("Complain - Distribui√ß√£o")
    st.pyplot(fig4)
    quantidade_reclamacoes = df4["Complain"].value_counts()
    st.write("Quantidade de Reclama√ß√µes: 0 (n√£o reclamou) e 1 (reclamou):")
    st.markdown("#### Quantidade de Reclama√ß√µes")
    st.markdown("0 (n√£o reclamou): {} | 1 (reclamou): {}".format(
        quantidade_reclamacoes.get(0, 0), quantidade_reclamacoes.get(1, 0)))
    st.markdown("#### Tabela de Reclama√ß√µes")
    quantidade_reclamacoes = df4["Complain"].value_counts().rename_axis("Complain").reset_index(name="Count")
    quantidade_reclamacoes["Percentage"] = (quantidade_reclamacoes["Count"] / quantidade_reclamacoes["Count"].sum()) * 100
    quantidade_reclamacoes["Percentage"] = quantidade_reclamacoes["Percentage"].round(2)
    quantidade_reclamacoes = quantidade_reclamacoes.rename(columns={"Complain": "Reclamou"})
    quantidade_reclamacoes = quantidade_reclamacoes.set_index("Reclamou")   
    st.write(quantidade_reclamacoes)    
    
    # Justifica que adotaremos a tecnica smote para lidar com o desbalanceamento
    st.markdown("### Desbalanceamento de Classes")
    st.write("A vari√°vel alvo `Complain` apresenta um desbalanceamento significativo, com a maioria dos clientes n√£o reclamando. Para lidar com isso, utilizaremos a t√©cnica SMOTE (Synthetic Minority Over-sampling Technique) para balancear as classes antes de treinar os modelos preditivos.")

    st.markdown("### üîç Boxplots: Vari√°veis num√©ricas vs Reclamar")
    col_numericas = ["Income", "Age", "TotalSpent", "TotalChildren", "Recency", "NumWebVisitsMonth"]

    for col in col_numericas:
        fig, ax = plt.subplots()
        sns.boxplot(x="Complain", y=col, data=df4, ax=ax)
        ax.set_title(f"{col} vs Reclamar")
        st.pyplot(fig)

    # ----------- Correla√ß√£o ----------- #
    st.markdown("## üìå Correla√ß√£o com Reclamar")
    correlacoes = df_encoded.corr()["Complain"].sort_values(ascending=False)
    st.dataframe(correlacoes.to_frame().rename(columns={"Complain": "Correla√ß√£o com Reclamar"}))

    # ----------- Dispers√£o Multivariada ----------- #
    st.markdown("## üåê Rela√ß√µes Multivariadas")

    fig, ax = plt.subplots()
    sns.scatterplot(x="Income", y="TotalSpent", hue="Complain", data=df4, ax=ax)
    ax.set_title("Gasto Total vs Renda (Colorido por Reclamar)")
    st.pyplot(fig)

    # Final
    st.success("‚úÖ An√°lise Descritiva Conclu√≠da.")


        
    

def q4_etapa3():
    st.header("Q4 - c) Sele√ß√£o de Modelos")
    st.info("Compare Logistic, √Årvores, Random Forest, XGBoost.")
    
    #recuperar df4 da sess√£o do streamlit
    df4 = st.session_state.get("df4")
    if df4 is None:
        st.warning("‚ö†Ô∏è Os dados ainda n√£o foram carregados. Execute a Etapa 2 primeiro.")
        return
   
    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.ensemble import RandomForestClassifier
    from xgboost import XGBClassifier
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, ConfusionMatrixDisplay
    from imblearn.over_sampling import SMOTE
    from sklearn.preprocessing import StandardScaler
    import warnings
    warnings.filterwarnings("ignore")
    # Pr√©-processamento
    df4["Income"] = df4["Income"].fillna(df4["Income"].median())
    df4["Age"] = 2025 - df4["Year_Birth"]
    df4["TotalChildren"] = df4["Kidhome"] + df4["Teenhome"]
    df4["TotalSpent"] = df4[[
        "MntWines", "MntFruits", "MntMeatProducts",
        "MntFishProducts", "MntSweetProducts", "MntGoldProds"
    ]].sum(axis=1)

    df4 = df4.drop(columns=["ID", "Year_Birth", "Dt_Customer", "Z_CostContact", "Z_Revenue"], errors="ignore")
    df4 = pd.get_dummies(df4, columns=["Education", "Marital_Status"], drop_first=True)

    # Divis√£o X e y
    X = df4.drop(columns=["Complain"])
    y = df4["Complain"]

    # Divis√£o treino/teste
    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

    # Escalonamento
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    



    # Modelos a testar
    modelos = {
        "√Årvore de Decis√£o": DecisionTreeClassifier(class_weight='balanced', random_state=42),
        "Random Forest": RandomForestClassifier(class_weight='balanced', random_state=42),
        "XGBoost": XGBClassifier(
            scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),
            use_label_encoder=False,
            eval_metric='logloss',
            random_state=42
        )
    }
    
     # Para avaliar o modelo na proxima questao
    
    #adicionar modelos na sess√£o do streamlit
    st.session_state["modelos"] = modelos
  
    #inserir X_test na sess√£o do streamlit
    st.session_state["X_test"] = X_test

  

    st.header("üîç Avalia√ß√£o dos Modelos")
    resultados = []

    for nome, modelo in modelos.items():
        modelo.fit(X_train_scaled, y_train)
        y_pred = modelo.predict(X_test_scaled)
        y_prob = modelo.predict_proba(X_test_scaled)[:, 1]

        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)
        auc = roc_auc_score(y_test, y_prob)

        # Matriz de confus√£o
        cm = confusion_matrix(y_test, y_pred)
        fig, ax = plt.subplots()
        disp = ConfusionMatrixDisplay(confusion_matrix=cm)
        disp.plot(ax=ax)
        plt.title(f"Matriz de Confus√£o - {nome}")
        st.pyplot(fig)

        resultados.append({
            "Modelo": nome,
            "Acur√°cia": report["accuracy"],
            "Precis√£o": report["1"]["precision"],
            "Recall": report["1"]["recall"],
            "F1-score": report["1"]["f1-score"],
            "AUC": auc
        })

    # Tabela final
    st.markdown("### Compara√ß√£o Final dos Modelos")
    df_result = pd.DataFrame(resultados).sort_values(by="F1-score", ascending=False)
    st.dataframe(df_result, use_container_width=True)
    
        
    st.markdown("### Sele√ß√£o de Modelos para Previs√£o de Reclama√ß√µes")
    
    st.markdown(
    "A seguir, apresentamos uma an√°lise comparativa de diferentes modelos de machine learning para prever reclama√ß√µes de clientes com base em suas caracter√≠sticas demogr√°ficas e comportamentais. "
    "Como a vari√°vel alvo `Complain` est√° altamente desbalanceada (apenas 22 positivos em mais de 2200 registros), aplicamos estrat√©gias espec√≠ficas de balanceamento para cada modelo:"
    )

    st.markdown("""
    - **√Årvore de Decis√£o**: utilizou `class_weight='balanced'` para penalizar mais os erros da classe minorit√°ria.
    - **Random Forest**: tamb√©m utilizou `class_weight='balanced'`, o que redistribui o peso das classes automaticamente.
    - **XGBoost**: utilizou `scale_pos_weight`, calculado como a raz√£o entre a classe negativa e a positiva, para ajustar a import√¢ncia da minoria.

    Essas abordagens buscam reduzir o vi√©s para a classe majorit√°ria e aumentar a sensibilidade do modelo aos clientes que realmente reclamaram.
    """)
    
    

    st.markdown(
        "A seguir, apresentamos uma an√°lise comparativa de diferentes modelos de machine learning para prever reclama√ß√µes de clientes com base em suas caracter√≠sticas demogr√°ficas e comportamentais. "
        "Os modelos testados foram: **√Årvore de Decis√£o**, **Random Forest** e **XGBoost**. "
        "O objetivo √© identificar o modelo mais eficaz para prever se um cliente ir√° reclamar ou n√£o, utilizando m√©tricas como acur√°cia, precis√£o, recall, F1-score e AUC."
    )

    st.markdown("""
    | Modelo             | Acur√°cia | Precis√£o | Recall | F1-Score | AUC     |
    |--------------------|----------|----------|--------|----------|---------|
    | XGBoost            | 0.993    | 0.667    | 0.400  | 0.500    | 0.704   |
    | Random Forest      | 0.993    | 1.000    | 0.200  | 0.333    | 0.771   |
    | √Årvore de Decis√£o  | 0.989    | 0.333    | 0.200  | 0.250    | 0.598   |
    """)

    st.markdown("""
    **An√°lise**:
    - O **Random Forest** apresentou o melhor desempenho em AUC (0.771), indicando boa capacidade de separa√ß√£o entre clientes que reclamam e os que n√£o reclamam.
    - O **XGBoost** teve o melhor equil√≠brio entre precis√£o e recall, resultando em maior F1-score (**0.500**), sendo mais robusto para detectar corretamente clientes com maior risco de reclamar.
    - A **√Årvore de Decis√£o** simples teve o desempenho mais baixo, sugerindo que modelos mais complexos s√£o mais adequados ao problema.

    ‚úÖ **Conclus√£o**: o modelo **XGBoost** √© o mais indicado, considerando o equil√≠brio entre todas as m√©tricas de avalia√ß√£o.
    """)
    
    
    st.markdown("### ‚úÖ Justificativa da escolha do Modelo XGBoost")

    st.markdown(
        "O **XGBoost (Extreme Gradient Boosting)** foi escolhido por sua alta capacidade de generaliza√ß√£o, efici√™ncia computacional e excelente desempenho em tarefas de classifica√ß√£o bin√°ria, especialmente em conjuntos de dados desbalanceados. "
        "Diferente de modelos tradicionais, o XGBoost permite o ajuste expl√≠cito do par√¢metro `scale_pos_weight`, que controla o peso da classe minorit√°ria no c√°lculo da fun√ß√£o de perda. "
        "Esse recurso √© especialmente √∫til no contexto desta an√°lise, onde apenas 1% dos clientes fizeram reclama√ß√µes, gerando forte desbalanceamento na vari√°vel alvo `Complain`."
    )

    st.markdown(
        "Al√©m disso, o XGBoost √© robusto contra overfitting e realiza regulariza√ß√£o L1/L2 automaticamente, o que o torna altamente indicado para dados com m√∫ltiplas vari√°veis explicativas e poss√≠veis correla√ß√µes."
    )

    st.success("üìå Portanto, o XGBoost foi ajustado com `scale_pos_weight` para lidar adequadamente com o desbalanceamento e demonstrou bom equil√≠brio entre precis√£o e recall.")

    


def q4_etapa4():
    st.header("Q4 - d) SHAP e Explicabilidade")
    st.info("Use SHAP para entender a influ√™ncia das vari√°veis.")
    

    # Recuperar o modelo XGBoost da sess√£o do streamlit
    modelos = st.session_state.get("modelos")
    if modelos is None:
        st.warning("‚ö†Ô∏è O modelo ainda n√£o foi treinado. Execute a Etapa 3 primeiro.")
        return

    X_test = st.session_state.get("X_test")
    if X_test is None:
        st.warning("‚ö†Ô∏è Os dados de teste ainda n√£o foram carregados. Execute a Etapa 3 primeiro.")
        return
    import shap
    import matplotlib.pyplot as plt
    # Aplicar SHAP (TreeExplainer para XGBoost)
    explainer = shap.TreeExplainer(modelos["XGBoost"])
    shap_values = explainer.shap_values(X_test)


    # T√≠tulo da se√ß√£o
    st.markdown("## üîç Explicabilidade das Vari√°veis com SHAP")

    st.markdown(
        """
        Utilizamos o m√©todo SHAP (SHapley Additive exPlanations) com o modelo XGBoost para entender o impacto de cada vari√°vel nas previs√µes de reclama√ß√µes.
        A seguir, s√£o exibidos dois gr√°ficos:
        - **Gr√°fico de barras**: mostra a import√¢ncia m√©dia das vari√°veis no modelo.
        - **Beeswarm**: mostra a distribui√ß√£o dos impactos individuais por vari√°vel.
        """
    )
    


    # Gr√°fico de barras (import√¢ncia m√©dia)
    st.subheader("üìä Import√¢ncia M√©dia das Vari√°veis")
    fig_bar, ax = plt.subplots()
    shap.summary_plot(shap_values, X_test, plot_type="bar", show=False)
    st.pyplot(fig_bar)

    # Gr√°fico beeswarm (impacto individual)
    st.subheader("üå™Ô∏è Impacto Individual das Vari√°veis")
    fig_beeswarm, ax = plt.subplots()
    shap.summary_plot(shap_values, X_test, show=False)
    st.pyplot(fig_beeswarm)
    
    st.markdown("### üìä Interpreta√ß√£o das Vari√°veis Mais Influentes (SHAP)")

    st.markdown("""
    A an√°lise de explicabilidade com SHAP revelou quais vari√°veis mais impactam a previs√£o de reclama√ß√µes dos clientes. Abaixo, discutimos os principais fatores e seu significado no contexto do neg√≥cio:
    """)

    st.markdown("""
    #### 1. Age (Idade)  
    A idade foi a vari√°vel com maior impacto no modelo preditivo.  
    Clientes mais jovens e mais velhos tendem a ter diferentes n√≠veis de expectativa em rela√ß√£o aos produtos e servi√ßos. Por exemplo, consumidores mais velhos podem ser mais criteriosos e mais propensos a formalizar reclama√ß√µes quando percebem falhas no atendimento ou no produto.
    """)

    st.markdown("""
    #### 2. MntWines (Gasto com vinhos)  
    Reflete o n√≠vel de consumo espec√≠fico em produtos premium como vinhos.  
    Clientes que investem valores mais altos nesse item s√£o geralmente mais exigentes quanto √† qualidade, entrega e experi√™ncia geral de compra, o que aumenta a chance de reclama√ß√µes em caso de frustra√ß√£o.
    """)

    st.markdown("""
    #### 3. MntMeatProducts & MntGoldProds  
    Indicadores de clientes com ticket m√©dio elevado.  
    Esses consumidores geralmente t√™m maior valor agregado para a empresa e, por isso, esperam um servi√ßo de excel√™ncia. Pequenas falhas podem comprometer sua experi√™ncia e lev√°-los a reclamar com mais frequ√™ncia.
    """)

    st.markdown("""
    #### 4. TotalSpent (Total gasto)  
    A soma total dos gastos em diferentes categorias mostra que quanto maior o investimento do cliente, maior sua aten√ß√£o √† jornada de consumo.  
    Se o retorno percebido (produto, atendimento, entrega) n√£o for proporcional ao valor investido, a probabilidade de reclama√ß√£o aumenta.
    """)

    st.markdown("""
    #### 5. Income (Renda)  
    A renda familiar est√° relacionada ao n√≠vel de exig√™ncia e expectativa.  
    Clientes de maior poder aquisitivo tendem a ser menos tolerantes a falhas operacionais e mais r√°pidos em expressar insatisfa√ß√£o por meio de reclama√ß√µes formais.
    """)

    st.markdown("### Conclus√£o")
    st.markdown("""
    O padr√£o identificado revela que clientes com alto engajamento, gastos expressivos e maior expectativa s√£o mais propensos a gerar reclama√ß√µes. 

    """)


def q4_etapa5():
    st.header("Q4 - e) K-Means / DBSCAN")
    st.info("Clusteriza√ß√£o e detec√ß√£o de outliers por perfil.")
        
 
    import pandas as pd
    from sklearn.preprocessing import StandardScaler
    from sklearn.cluster import KMeans, DBSCAN
    import matplotlib.pyplot as plt

    # T√≠tulo da se√ß√£o
    st.markdown("## An√°lise N√£o Supervisionada - K-Means (Segmenta√ß√£o de Clientes)")

    # Carregar a base
    df4 = st.session_state.get("df4")
    if df4 is None:
        st.warning("‚ö†Ô∏è Os dados ainda n√£o foram carregados. Execute a Etapa 2 primeiro.")
        return

    # Sele√ß√£o das colunas para clusteriza√ß√£o
    colunas_cluster = [
        "Income", "Kidhome", "Teenhome", "MntWines", "MntFruits",
        "MntMeatProducts", "MntFishProducts", "MntSweetProducts",
        "MntGoldProds", "NumDealsPurchases", "NumWebPurchases",
        "NumCatalogPurchases", "NumStorePurchases", "NumWebVisitsMonth"
    ]

    # Remover valores ausentes
    df_cluster = df4[colunas_cluster].dropna()

    # Padronizar os dados
    scaler = StandardScaler()
    X_cluster = scaler.fit_transform(df_cluster)

    # Calcular WCSS para diferentes valores de k
    wcss = []
    for k in range(1, 10):
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X_cluster)
        wcss.append(kmeans.inertia_)

    # Exibir o gr√°fico Elbow
    st.markdown("### M√©todo do Cotovelo")
    fig, ax = plt.subplots(figsize=(5, 3))
    ax.plot(range(1, 10), wcss, marker='o')
    ax.set_xlabel("N√∫mero de Clusters (k)")
    ax.set_ylabel("Soma dos Erros Quadr√°ticos (WCSS)")
    ax.set_title("Elbow - Defini√ß√£o do N√∫mero Ideal de Clusters")
    ax.grid(True)
    st.pyplot(fig)

    
        
        # Aplicar K-Means com k=3
    st.markdown("### Segmenta√ß√£o de Clientes com K-Means (k=3)")
    st.info("Segmenta√ß√£o dos clientes em 3 grupos com base nos atributos selecionados.")
    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(X_cluster)

    # Adicionar os clusters ao dataframe original
    df_cluster["Cluster"] = clusters

    # Mostrar a contagem de clientes por cluster
    st.markdown("### Quantidade de Clientes por Cluster")
    st.dataframe(df_cluster["Cluster"].value_counts().rename("Clientes").reset_index().rename(columns={"index": "Cluster"}))

    # Exibir m√©dias por cluster
    st.markdown("### M√©dias dos Atributos por Cluster")
    st.dataframe(df_cluster.groupby("Cluster").mean().round(2))
        
    st.markdown("### Justificativa do N√∫mero de Clusters k=3")
    st.markdown("""
    A an√°lise do gr√°fico Elbow (cotovelo) mostra uma redu√ß√£o acentuada na soma dos erros quadr√°ticos (WCSS) entre os valores de **k = 1 at√© k = 3**. A partir de **k = 4**, a queda no WCSS se torna mais sutil, indicando que os ganhos marginais com a adi√ß√£o de novos clusters s√£o pequenos.
    Portanto, o ponto de inflex√£o do gr√°fico ocorre em **k = 3**, sugerindo que **3 clusters representam um bom compromisso entre simplicidade e capacidade explicativa do modelo**.
    """)

        
    

    # Gr√°fico de dispers√£o de dois atributos para visualiza√ß√£o
    st.markdown("### Visualiza√ß√£o dos Clusters (Ex: Income vs MntWines)")
    fig, ax = plt.subplots()
    sns.scatterplot(data=df_cluster, x="Income", y="MntWines", hue="Cluster", palette="Set1", ax=ax)
    ax.set_title("Segmenta√ß√£o dos Clientes por K-Means (k=3)")
    st.pyplot(fig)
        
    st.markdown("### Interpreta√ß√£o dos Perfis - Agrupamento com K-Means (3 Clusters)")

    st.markdown("""
    Ap√≥s aplicar o algoritmo **K-Means com 3 clusters**, foi poss√≠vel identificar tr√™s grupos distintos de clientes com base em caracter√≠sticas como renda, padr√£o de consumo, estrutura familiar e comportamento de compra. 
    A an√°lise das m√©dias dessas vari√°veis permitiu classificar os grupos em perfis interpret√°veis do ponto de vista de neg√≥cios e segmenta√ß√£o de mercado.
    """)

    st.markdown("""
    - **Cluster 0 ‚Äì Perfil M√©dio**  
    Clientes com **renda intermedi√°ria** (58 mil), **n√≠vel de consumo moderado** e com filhos em idade escolar e adolescentes (Kidhome: 0.23 / Teenhome: 0.93).  
    Demonstram comportamento de compra equilibrado entre canais f√≠sicos e digitais, com boa frequ√™ncia de compras online (NumWebPurchases: 6.37) e gasto razo√°vel com vinhos e carnes.  
    Representam um p√∫blico de **classe m√©dia engajada**, com potencial de fideliza√ß√£o e boa aceita√ß√£o a promo√ß√µes.

    - **Cluster 1 ‚Äì Perfil Premium/Alta Renda**  
    Grupo com **maior renda m√©dia (76 mil)** e **alt√≠ssimo consumo** em todas as categorias, especialmente vinhos (589), carnes (454) e doces (71).  
    S√£o clientes com **poucos filhos**, muito ativos no canal de cat√°logo (NumCatalogPurchases: 5.98) e lojas f√≠sicas (NumStorePurchases: 8.4).  
    Refletem um perfil **exigente, fiel e de alto valor** para a empresa, devendo ser priorizados em estrat√©gias de reten√ß√£o e atendimento diferenciado.

    - **Cluster 2 ‚Äì Perfil Econ√¥mico**  
    Apresentam a **menor renda m√©dia (35 mil)** e **baix√≠ssimo consumo**, com destaque para vinhos (42), carnes (23) e frutas (4).  
    T√™m mais filhos (Kidhome: 0.8 / Teenhome: 0.45) e s√£o pouco engajados em canais digitais (NumWebPurchases: 2.12), usando mais o site apenas para visita (NumWebVisitsMonth: 6.47).  
    Representam clientes **sens√≠veis ao pre√ßo**, com foco em necessidades b√°sicas e menor frequ√™ncia de compra.
    """)
        
    # Aplicar DBSCAN
    dbscan = DBSCAN(eps=1.6, min_samples=5)
    dbscan_labels = dbscan.fit_predict(X_cluster)
    df4["DBSCAN_Cluster"] = dbscan_labels

    # Exibir resultados
    st.subheader("Detec√ß√£o de Perfis At√≠picos com DBSCAN")
    st.markdown("A t√©cnica DBSCAN foi aplicada para detectar grupos de clientes e perfis at√≠picos (outliers).")

    # Contagem de clusters e outliers
    outlier_count = (df4["DBSCAN_Cluster"] == -1).sum()
    st.write("Total de registros:", len(df4))
    st.write("Total de outliers detectados:", outlier_count)
    st.write("Grupos encontrados (DBSCAN_Cluster):")
    st.dataframe(df4["DBSCAN_Cluster"].value_counts().rename("Quantidade").to_frame())

    st.markdown("## Integra√ß√£o entre Agrupamentos e Modelos Supervisionados")

    st.markdown("""
    A uni√£o entre os agrupamentos (**K-Means** e **DBSCAN**) e os modelos supervisionados forma uma abordagem mais robusta para a an√°lise de comportamento e previs√£o de reclama√ß√µes de clientes.

    - **K-Means** auxilia na **estratifica√ß√£o de clientes por perfil**, permitindo identificar em quais segmentos as reclama√ß√µes se concentram. Complemento √† supervis√£o: ao observar a propor√ß√£o de Complain em cada cluster, pode-se verificar quais perfis concentram mais insatisfa√ß√£o, mesmo que a vari√°vel n√£o tenha sido usada no agrupamento. Isso refor√ßa a explica√ß√£o dos modelos supervisionados e permite a√ß√µes direcionadas por perfil.

    - **DBSCAN** contribui ao **detectar anormalidades relevantes**, como clientes com comportamento fora do padr√£o esperado. Esses outliers podem representar casos de frustra√ß√£o, desengajamento ou experi√™ncias excepcionais (positivas ou negativas).

    - Os **modelos supervisionados** indicam, com boa acur√°cia, **quem tende a reclamar**, mas **se beneficiam ao serem interpretados em conjunto com os perfis descobertos nos clusters**, trazendo mais contexto e precis√£o √†s a√ß√µes estrat√©gicas.

    ‚úÖ Essa integra√ß√£o amplia a capacidade da empresa de **antecipar problemas, personalizar o atendimento e fidelizar clientes** com base em evid√™ncias estat√≠sticas e comportamentais.
    """)

def q4_etapa6():
    st.header("Q4 - f) Decis√£o Estrat√©gica")
    st.info("Sugira melhorias com base nos insights obtidos.")
    
    st.markdown("### Decis√µes Estrat√©gicas e Implica√ß√µes para o Neg√≥cio")

    st.markdown("""
    A segmenta√ß√£o dos clientes em tr√™s clusters fornece **insumos estrat√©gicos valiosos** para a√ß√µes de marketing, fideliza√ß√£o e personaliza√ß√£o do atendimento.

    - **Cluster 1 (Premium)** deve ser o foco de **a√ß√µes de fideliza√ß√£o e reten√ß√£o personalizadas**, como programas de recompensas, atendimento exclusivo e ofertas especiais. Esses clientes possuem alto ticket m√©dio, forte engajamento e grande potencial de gera√ß√£o de receita.

    - **Cluster 0 (Classe M√©dia)** representa um p√∫blico com bom n√≠vel de consumo e engajamento. Estrat√©gias como **promo√ß√µes direcionadas, upgrades de produtos e cross-selling** podem aumentar ainda mais o seu valor ao longo do tempo. √â um grupo estrat√©gico para **crescimento sustent√°vel** da base de clientes.

    - **Cluster 2 (Econ√¥mico)** √© mais sens√≠vel ao pre√ßo e menos engajado digitalmente. Para esse grupo, a√ß√µes de **inclus√£o, ofertas acess√≠veis, campanhas educativas e canais presenciais** podem ser mais eficazes. Embora tenham menor valor individual, sua quantidade pode representar uma **base volumosa e relevante**.

    A segmenta√ß√£o permite que a empresa **adapte sua comunica√ß√£o e servi√ßos a cada perfil**, otimizando investimentos e aumentando a satisfa√ß√£o do cliente. Al√©m disso, √© poss√≠vel **cruzar os clusters com vari√°veis como reclama√ß√µes e churn** para priorizar melhorias e evitar perdas de clientes estrat√©gicos.
    """)
    
    st.markdown("### Aplica√ß√µes Estrat√©gicas da An√°lise de Dados")

    st.markdown("""
    A an√°lise de dados realizada oferece **insumos valiosos para a tomada de decis√£o estrat√©gica**, especialmente nas seguintes frentes:

    #### Reten√ß√£o de Clientes
    - A identifica√ß√£o de perfis mais propensos a reclamar permite **a√ß√µes preventivas**, como contato proativo, ofertas de fideliza√ß√£o ou acompanhamento personalizado.
    - Clusters com maior taxa de insatisfa√ß√£o podem ser alvo de **programas de engajamento** espec√≠ficos para evitar o churn.

    #### Melhoria do Suporte ao Cliente
    - Vari√°veis como idade, renda e canais de compra ajudam a **adaptar o atendimento ao perfil do cliente**.
    - Clientes premium ou com alto ticket m√©dio devem receber **suporte priorit√°rio e especializado**, aumentando a satisfa√ß√£o e o valor percebido.

    #### Personaliza√ß√£o de Produtos e Servi√ßos
    - A segmenta√ß√£o permite **criar campanhas de marketing personalizadas**, com base em h√°bitos de consumo (ex: vinhos, carnes, cat√°logo).
    - Estrat√©gias diferenciadas podem ser definidas para cada grupo identificado (econ√¥mico, m√©dio, premium), aumentando a **efic√°cia das a√ß√µes comerciais**.

    ‚úÖ Em resumo, a combina√ß√£o de modelos preditivos e agrupamentos n√£o supervisionados fornece uma vis√£o ampla e acion√°vel sobre o comportamento do cliente, permitindo uma **gest√£o mais inteligente da base de clientes** e **aumento da competitividade no mercado**.
    """)

        
    


# =============================
# ‚ñ∂Ô∏è APP Streamlit
# =============================
st.set_page_config(page_title="üìä Prova Final - An√°lise Estat√≠stica", layout="wide")
st.title("üìö Prova Final - An√°lise Estat√≠stica de Dados e Informa√ß√µes")
st.markdown("Desenvolvido por: [Silvia Laryssa Branco da Silva] &nbsp;&nbsp;&nbsp;&nbsp;üìÖ Julho 2025")
st.markdown("""
### üìÑ Acesse a prova final

Clique no link abaixo para visualizar e interagir com o painel da prova final:

üîó [üëâ AIED - Prova Final: ](https://aiedprovafinal.streamlit.app/)
""")


# MENU LATERAL
with st.sidebar:
    st.title("üß≠ Menu da Prova")
    mostrar_todas = st.checkbox("‚úÖ Mostrar todas as quest√µes", value=False)

    # Quest√£o 1
    with st.expander("üè® Quest√£o 1 - Regress√£o Linear (Im√≥veis)", expanded=mostrar_todas):
        show_q1_e1 = mostrar_todas or st.checkbox("1Ô∏è An√°lise Descritiva", key="q1e1")
        show_q1_e2 = mostrar_todas or st.checkbox("2Ô∏è Modelo de Regress√£o Linear", key="q1e2")
        show_q1_e3 = mostrar_todas or st.checkbox("3Ô∏è Interpreta√ß√£o dos Resultados", key="q1e3")
        show_q1_e4 = mostrar_todas or st.checkbox("4Ô∏è Ajustes no Modelo", key="q1e4")
        show_q1_e5 = mostrar_todas or st.checkbox("5Ô∏è Tomada de Decis√£o", key="q1e5")

    # Quest√£o 2
    with st.expander("üè® Quest√£o 2 - Regress√£o Log√≠stica (Reservas)", expanded=mostrar_todas):
        show_q2_e1 = mostrar_todas or st.checkbox("a) An√°lise Descritiva", key="q2e1")
        show_q2_e2 = mostrar_todas or st.checkbox("b) Modelo de Regress√£o Log√≠stica", key="q2e2")
        show_q2_e3 = mostrar_todas or st.checkbox("c) An√°lise das Features", key="q2e3")
        show_q2_e4 = mostrar_todas or st.checkbox("d) Justificativa do M√©todo", key="q2e4")

    # Quest√£o 3
    with st.expander("üè® Quest√£o 3 - ANOVA (Vendas por Pa√≠s)", expanded=mostrar_todas):
        show_q3_e1 = mostrar_todas or st.checkbox("a) An√°lise Descritiva", key="q3e1")
        show_q3_e2 = mostrar_todas or st.checkbox("b) ANOVA entre Pa√≠ses", key="q3e2")
        show_q3_e3 = mostrar_todas or st.checkbox("c) Ajustes no Modelo", key="q3e3")
        show_q3_e4 = mostrar_todas or st.checkbox("d) Interpreta√ß√£o e Decis√£o", key="q3e4")

    # Quest√£o 4
    with st.expander("üè® Quest√£o 4 - Reclama√ß√µes de Clientes", expanded=mostrar_todas):
        show_q4_e1 = mostrar_todas or st.checkbox("a) Discuss√£o do Problema", key="q4e1")
        show_q4_e2 = mostrar_todas or st.checkbox("b) An√°lise Descritiva", key="q4e2")
        show_q4_e3 = mostrar_todas or st.checkbox("c) Sele√ß√£o de Modelos", key="q4e3")
        show_q4_e4 = mostrar_todas or st.checkbox("d) SHAP e Explicabilidade", key="q4e4")
        show_q4_e5 = mostrar_todas or st.checkbox("e) K-Means / DBSCAN", key="q4e5")
        show_q4_e6 = mostrar_todas or st.checkbox("f) Decis√£o Estrat√©gica", key="q4e6")

# =============================
# ‚ñ∂Ô∏è EXECU√á√ÉO DE ETAPAS SELECIONADAS
# =============================

if show_q1_e1: q1_etapa1()
if show_q1_e2: q1_etapa2()
if show_q1_e3: q1_etapa3()
if show_q1_e4: q1_etapa4()
if show_q1_e5: q1_etapa5()

if show_q2_e1: q2_etapa1()
if show_q2_e2: q2_etapa2()
if show_q2_e3: q2_etapa3()
if show_q2_e4: q2_etapa4()

if show_q3_e1: q3_etapa1()
if show_q3_e2: q3_etapa2()
if show_q3_e3: q3_etapa3()
if show_q3_e4: q3_etapa4()

if show_q4_e1: q4_etapa1()
if show_q4_e2: q4_etapa2()
if show_q4_e3: q4_etapa3()
if show_q4_e4: q4_etapa4()
if show_q4_e5: q4_etapa5()
if show_q4_e6: q4_etapa6()

# Rodap√©
st.markdown("---")
st.markdown("üßÆ **Prova Final - AEDI - PPCA - UNB**  \nüë©‚Äçüè´ Professor(a): Jo√£o Gabriel de Moraes Souza  \nüìä Universidade de Brasilia")

st.markdown("### Refer√™ncias Bibliogr√°ficas")

st.markdown("""
- CALLEGARI-JACQUES, Sidia Maria. *Bioestat√≠stica: Princ√≠pios e Aplica√ß√µes*. Porto Alegre: Artmed, 2007.

- FIELD, Andy. *Descobrindo a Estat√≠stica Usando o SPSS*. Tradu√ß√£o de Regina Machado. Porto Alegre: Artmed, 2009.

- HUFF, Darrell. *Como Mentir com Estat√≠stica*. S√£o Paulo: Edi√ß√µes Bookman, 2009.

- MAGALH√ÉES, Marcos Nascimento; LIMA, Antonio Carlos Pedroso de. *No√ß√µes de Probabilidade e Estat√≠stica*. S√£o Paulo: Edusp, 2004.

- G√âRON, Aur√©lien. *M√£os √† Obra: Aprendizado de M√°quina com Scikit-Learn, Keras e TensorFlow*. Tradu√ß√£o de Pedro Jatob√°. Rio de Janeiro: Alta Books, 2021.

- GRUS, Joel. *Data Science do Zero: No√ß√µes Fundamentais com Python*. Tradu√ß√£o de Juliana D. Ferreira. Rio de Janeiro: Alta Books, 2021.
""")

